{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJWDq-8og-mX"
   },
   "source": [
    "# Pytorch setup, imports and constants initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "fYAFESqjg8fM",
    "outputId": "d4bf9888-26b4-4b48-b172-6c35ccb997ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.0.1\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl (614.8MB)\n",
      "\u001b[K     |████████████████████████████████| 614.8MB 28kB/s \n",
      "\u001b[31mERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 1.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "  Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "Successfully installed torch-1.0.1\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.2)\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "!pip3 install https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl\n",
    "!pip3 install torch torchvision\n",
    "  \n",
    "import torch\n",
    "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "NK4-jWrThJd5",
    "outputId": "7bc4d9fa-71fb-4e89-9ebb-f9873a613c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymagnitude\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/a3/b9a34d22ed8c0ed59b00ff55092129641cdfa09d82f9abdc5088051a5b0c/pymagnitude-0.1.120.tar.gz (5.4MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4MB 4.4MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pymagnitude\n",
      "  Building wheel for pymagnitude (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pymagnitude: filename=pymagnitude-0.1.120-cp36-cp36m-linux_x86_64.whl size=135918206 sha256=f68cca37e6356fea3d51d92249d4578a2d5c260a0c70971477d0c3d22e6ace1d\n",
      "  Stored in directory: /root/.cache/pip/wheels/a2/c7/98/cb48b9db35f8d1a7827b764dc36c5515179dc116448a47c8a1\n",
      "Successfully built pymagnitude\n",
      "Installing collected packages: pymagnitude\n",
      "Successfully installed pymagnitude-0.1.120\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pymagnitude\n",
    "from pymagnitude import *\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from sklearn.metrics import f1_score\n",
    "from pdb import set_trace as debug\n",
    "\n",
    "\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "BOS = 2\n",
    "EOS = 3\n",
    "\n",
    "PAD_WORD = '<blank>'\n",
    "UNK_WORD = '<unk>'\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "\n",
    "CAT = ['PER', 'ORG', 'LOC', 'MISC']\n",
    "POSITION = ['I', 'B']\n",
    "LABEL_INDEX = [PAD_WORD] + ['O'] + [\"{}-{}\".format(position, cat) for cat in CAT for position in POSITION]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvgAcjm6hyiH"
   },
   "source": [
    "# Connect to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "0YfhNBjSynDh",
    "outputId": "8c6a0345-e2d4-41cd-ccf1-2e6ff55cc019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n",
      "/content/gdrive/Shared drives/CIS 530 Project\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "%cd \"/content/gdrive/Shared drives/CIS 530 Project/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vZxser1ahU72"
   },
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhafrdBfhXZo"
   },
   "outputs": [],
   "source": [
    "# Returns the dictionary from the file with translations\n",
    "def english_to_spanish_dict(path):\n",
    "  en_to_spanish = dict()\n",
    "\n",
    "  with open(path, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n').split('\\t')\n",
    "        en_word = line[0]\n",
    "        es_word = line[1]\n",
    "        if en_word.isupper(): en_to_spanish[en_word] = es_word.upper()\n",
    "        elif len(en_word) > 0 and en_word[0].isupper(): en_to_spanish[en_word] = es_word.capitalize()\n",
    "        else: en_to_spanish[en_word] = es_word\n",
    "\n",
    "  return en_to_spanish\n",
    "\n",
    "# Returns a tuple (matrix, word_to_index) where the matrix contains all the \n",
    "# X-dimensional word embeddings, and word_to_index is a dictionary from the\n",
    "# word into the index in the matrix. For Out-of-vocabulary words it creates\n",
    "# a random embedding\n",
    "def get_indexed_word_embeddings(translation_path, embedding_path):\n",
    "  en_to_spanish = english_to_spanish_dict(translation_path)\n",
    "  vectors = Magnitude(embedding_path)\n",
    "\n",
    "  dim = vectors.dim\n",
    "  \n",
    "  matrix = [] # words by embedding-dimension\n",
    "  word_to_index = dict()\n",
    "\n",
    "  index = 0\n",
    "\n",
    "  en_files = ['Data/eng.train', 'Data/eng.testa', 'Data/eng.testb']\n",
    "  \n",
    "  for file in en_files:\n",
    "    with open(file, 'r') as f:\n",
    "      for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        if line == \"\": continue\n",
    "\n",
    "        word = line.split()[0]\n",
    "        trans = en_to_spanish[word]\n",
    "        if trans not in word_to_index:\n",
    "          word_to_index[trans] = index\n",
    "          if trans in vectors: matrix.append(vectors.query(trans))\n",
    "          else: matrix.append(np.random.uniform(-(3/dim)**0.5, (3/dim)**0.5, dim))\n",
    "\n",
    "          index += 1\n",
    "\n",
    "  es_files = ['Data/esp.train', 'Data/esp.testa', 'Data/esp.testb']\n",
    "  \n",
    "  for file in es_files:\n",
    "    with open(file, 'r') as f:\n",
    "      for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        if line == \"\": continue\n",
    "\n",
    "        word = line.split()[0]\n",
    "        if word not in word_to_index:\n",
    "          word_to_index[word] = index\n",
    "          if word in vectors: matrix.append(vectors.query(word))\n",
    "          else: matrix.append(np.random.uniform(-(3/dim)**0.5, (3/dim)**0.5, dim))\n",
    "            \n",
    "          index += 1\n",
    "\n",
    "  return matrix, word_to_index\n",
    "\n",
    "# Returns char dictionary created from all path in paths\n",
    "def create_char_index(paths, es_translation_dict, pad=False): #AS IS\n",
    "    char_dict = {}\n",
    "    if pad:\n",
    "        char_dict[PAD_WORD] = PAD\n",
    "        char_dict[UNK_WORD] = UNK\n",
    "    else:\n",
    "        char_dict[UNK_WORD] = 0\n",
    "\n",
    "    for path in paths:\n",
    "        for line in open(path):\n",
    "            l = line.strip().split()\n",
    "            if len(l) > 0:# and l[0] != '':\n",
    "              #l[0] is word l[1] is POS, l[2] is gold standard NER label\n",
    "              word = l[0]\n",
    "              es_word = word\n",
    "              if word in es_translation_dict:\n",
    "                es_word = es_translation_dict.get(word)\n",
    "              for i in range(len(es_word)):\n",
    "                  if es_word[i] not in char_dict:\n",
    "                      char_dict[es_word[i]] = len(char_dict)\n",
    "\n",
    "    return char_dict\n",
    "\n",
    "#   Returns \n",
    "#1. all the spanish 'sentences' in 2D array\n",
    "#2. 2D array indicating if word in given sentence is OOV word (True if the word is used as-is, translation not found) or not\n",
    "#3. 2D array of labels.\n",
    "def data_to_words_sentences(paths, es_translation_dict, test=False):\n",
    "  sentences=[]\n",
    "  curr_sentence=[]\n",
    "  OOV = []\n",
    "  labels=[]\n",
    "  curr_OOV_sentence = []\n",
    "  curr_label_sentence = []\n",
    "  word_idx = 0\n",
    "  for path in paths:\n",
    "      for line in open(path):\n",
    "        line = line.strip().split()\n",
    "        \n",
    "        end_of_line=False\n",
    "        if len(line) == 0:\n",
    "          end_of_line=True\n",
    "        \n",
    "        if not end_of_line:\n",
    "          word = line[0]\n",
    "          es_word = word\n",
    "\n",
    "          if not test: #english has 4th column label\n",
    "            curr_label_sentence.append(line[3])\n",
    "          else: #spanish has 3rd column label\n",
    "            curr_label_sentence.append(line[2])\n",
    "\n",
    "          curr_OOV_sentence.append(True)\n",
    "          if test:\n",
    "            curr_OOV_sentence[word_idx] = False\n",
    "          else: #not test\n",
    "            if word in es_translation_dict:\n",
    "              es_word = es_translation_dict.get(word)\n",
    "              curr_OOV_sentence[word_idx] = False\n",
    "          \n",
    "          word_idx = word_idx+1\n",
    "          curr_sentence.append(es_word)\n",
    "        if end_of_line:\n",
    "          sentences.append(curr_sentence)\n",
    "          OOV.append(curr_OOV_sentence)\n",
    "          labels.append(curr_label_sentence)\n",
    "          curr_sentence=[]\n",
    "          curr_OOV_sentence=[]\n",
    "          curr_label_sentence=[]\n",
    "          word_idx=0\n",
    "  return sentences, OOV, labels\n",
    "\n",
    "# Contains all logic to get initial input of neural network. Returns char, label, and word input\n",
    "# OOV[i] indicates if word[i] is OOV (translation not found)\n",
    "def get_input(word_vocab_dict, sentences, OOV, char_vocab_dict, token_labels, label_to_index):\n",
    "  max_word_len = max(len(word) for sentence in sentences for word in sentence)\n",
    "  #max_word_len = max(len(word) for word in words)\n",
    "  max_sentence_length = max(len(sentence) for sentence in sentences)\n",
    "\n",
    "  word_input = np.zeros((len(sentences), max_sentence_length), dtype='int64')\n",
    "  word_input_length = [len(sentence) for sentence in sentences]\n",
    "\n",
    "  char_input = np.zeros((len(sentences), max_sentence_length,max_word_len), dtype='int64') \n",
    "  char_input_length = np.zeros((len(sentences), max_sentence_length), dtype='int64') #2D array of length of word in each sentence in sentences\n",
    "  \n",
    "  label_input = np.zeros((len(sentences), max_sentence_length), dtype='int64') #2D array of label of word in each sentence in sentences\n",
    "  for i in range(len(sentences)):\n",
    "    for j in range(len(sentences[i])):\n",
    "      word_input[i][j] = word_vocab_dict[sentences[i][j]]\n",
    "      char_input_length[i][j] = len(sentences[i][j]) \n",
    "      label_input[i][j] = label_to_index.index(token_labels[i][j])\n",
    "      for k in range(len(sentences[i][j])):\n",
    "        c = sentences[i][j][k]\n",
    "        if c in char_vocab_dict:\n",
    "          input_zero = c.isdigit() or OOV[i][j]\n",
    "          char_input[i][j][k] = char_vocab_dict['0' if input_zero else c]\n",
    "        else:\n",
    "          char_input[i][j][k] = UNK\n",
    "\n",
    "  word_input_var = Variable(torch.from_numpy(word_input), requires_grad=False)\n",
    "  word_input_length_var = Variable(torch.LongTensor(word_input_length), requires_grad=False)\n",
    "  label_var = Variable(torch.from_numpy(label_input), requires_grad=False)\n",
    "  char_input_var = Variable(torch.from_numpy(char_input), requires_grad=False)\n",
    "  char_input_length_var = Variable(torch.from_numpy(char_input_length), requires_grad=False)\n",
    "  return word_input_var.cuda(), word_input_length_var.cuda(), char_input_var.cuda(), char_input_length_var.cuda(), label_var.cuda()\n",
    "\n",
    "def batch_from_data(X, X1, y, batch_size, random=True):\n",
    "    batch_num = int(np.ceil(len(y) / float(batch_size)))\n",
    "    rand_indices = np.arange(len(y))\n",
    "    if random: rand_indices = np.random.permutation(len(y))\n",
    "\n",
    "    for batch in range(0, batch_num):\n",
    "        bs = batch_size if batch < batch_num - 1 else len(y) - batch_size * batch\n",
    "        #from pdb import set_trace as debug\n",
    "        #debug()\n",
    "        yield [X[i] for i in rand_indices[batch * batch_size : batch * batch_size + bs]], [X1[i] for i in rand_indices[batch * batch_size : batch * batch_size + bs]], [y[i] for i in rand_indices[batch * batch_size : batch * batch_size + bs]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wpY4fK3phkHJ"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gzIbKaR0bph_"
   },
   "source": [
    "##char embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FPyhlkQQhmfg"
   },
   "outputs": [],
   "source": [
    "class char_model(nn.Module):\n",
    "    def __init__(self, char_vocab_size, char_embed_size, char_lstm_hidden_size=50):\n",
    "\n",
    "        #START: char embedding section\n",
    "        super(char_model, self).__init__()\n",
    "        self.char_embed = nn.Embedding(char_vocab_size, char_embed_size, padding_idx=PAD)\n",
    "        self.char_lstm = nn.LSTM(char_embed_size, char_lstm_hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.char_lstm_hidden_size = char_lstm_hidden_size\n",
    "        #END: char embedding section \n",
    "\n",
    "    def forward(self, char_inp, char_input_length):\n",
    "\n",
    "        #START: char embedding section\n",
    "        #from pdb import set_trace as debug\n",
    "        #debug()\n",
    "        char_input = char_inp.view(-1, char_inp.size(2))\n",
    "        char_input_length_sorted, char_original_idx = char_input_length.view(-1).sort(0, descending=True)\n",
    "        char_embedded = self.char_embed(char_input)\n",
    "        char_embedded_sorted = char_embedded[char_original_idx] #get embeddings in descending order of length of word in words\n",
    "\n",
    "        char_input_length_sorted_size = char_input_length_sorted.size(0)\n",
    "        last_index = char_input_length_sorted_size\n",
    "        if char_input_length_sorted.data.eq(0).sum() != 0: #atleast 1 element of char_input_length_sorted is/are zero\n",
    "          last_index = char_input_length_sorted.data.eq(0).nonzero()[0][0]\n",
    "        char_embedded_sorted = char_embedded_sorted[:last_index]\n",
    "        char_input_length_sorted = char_input_length_sorted[:last_index]\n",
    "\n",
    "        char_input_packed_padded = pack_padded_sequence(char_embedded_sorted, char_input_length_sorted.cpu().data.numpy(), batch_first=True)\n",
    "        char_output_packed_padded, (h_n, c_n) = self.char_lstm(char_input_packed_padded)\n",
    "        char_hidden_state = torch.cat([h_n[0], h_n[1]], 1)\n",
    "\n",
    "        if last_index != char_input_length_sorted_size:\n",
    "          zero_padding_diff = char_input_length_sorted_size - last_index\n",
    "          zero_padding = Variable(torch.zeros((zero_padding_diff, 2*self.char_lstm_hidden_size)), requires_grad=False).cuda()\n",
    "          char_hidden_state = torch.cat([char_hidden_state, zero_padding], 0)\n",
    "\n",
    "        char_hidden_state = char_hidden_state[torch.argsort(char_original_idx)] #char_hidden_state[torch.from_numpy(np.argsort(char_original_idx.cpu().data.numpy())).cuda()]\n",
    "        char_hidden_state = char_hidden_state.view(char_inp.size(0), -1, char_hidden_state.size(1))\n",
    "        return char_hidden_state\n",
    "        #END: char embedding section \n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "      for param in self.char_embed.parameters():\n",
    "          nn.init.normal(param, mean=0, std=0.01)\n",
    "\n",
    "      for name, param in self.char_lstm.named_parameters():\n",
    "          if 'bias' in name:\n",
    "              nn.init.constant_(param, 0.)\n",
    "          elif 'weight' in name:\n",
    "              nn.init.normal(param, mean=0, std=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DY26JE0ebtmV"
   },
   "source": [
    "##Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evmwaqT6-fal"
   },
   "outputs": [],
   "source": [
    "class selfAttention(nn.Module):\n",
    "\n",
    "  def __init__(self, char_lstm_hidden_size, word_vocab_size, word_embedding_size, word_lstm_hiddden_size, word_vector):\n",
    "    super(selfAttention, self).__init__()\n",
    "    self.word_embed = nn.Embedding(word_vocab_size, word_embedding_size, padding_idx=PAD)\n",
    "    self.word_lstm = nn.LSTM(2*char_lstm_hidden_size + word_embedding_size, word_lstm_hiddden_size, batch_first=True, bidirectional=True)\n",
    "\n",
    "    self.word_linear = nn.Linear(word_lstm_hiddden_size * 2, word_lstm_hiddden_size * 2)\n",
    "\n",
    "    self.tanh = nn.Tanh()\n",
    "    self.softmax = nn.Softmax(dim=2)\n",
    "    self.embedding_dropout = nn.Dropout(0.5)\n",
    "    self.word_dropout = nn.Dropout(0.5)\n",
    "    self.att_sm_dropout = nn.Dropout(0.5)\n",
    "    #self.att_dropout= nn.Dropout(0.2)\n",
    "    self.word_embed.weight.data.copy_(torch.from_numpy(np.asarray(word_vector)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, words, char_hidden_state, word_length):\n",
    "    word_embedding = self.word_embed(words)\n",
    "    word_lstm_input = torch.cat([word_embedding, char_hidden_state], 2)\n",
    "\n",
    "    word_lstm_input = self.embedding_dropout(word_lstm_input)\n",
    "    \n",
    "    word_length, word_idx = word_length.sort(0, descending=True)\n",
    "    word_lstm_input = word_lstm_input[word_idx]\n",
    "\n",
    "    word_packed_input = pack_padded_sequence(word_lstm_input, word_length.cpu().data.numpy(), batch_first=True)\n",
    "    word_packed_output, _ = self.word_lstm(word_packed_input)\n",
    "    word_output, _ = pad_packed_sequence(word_packed_output, batch_first=True)\n",
    "    word_output = word_output[torch.from_numpy(np.argsort(word_idx.cpu().data.numpy())).cuda()]\n",
    "    \n",
    "    word_output = self.word_dropout(word_output)\n",
    "    attn_input = self.tanh(self.word_linear(word_output))\n",
    "\n",
    "    att_padding_mask = Variable(words.data.ne(PAD)).cuda()\n",
    "    context = attn_input * att_padding_mask.float().unsqueeze(2)\n",
    "    attn_out = context.bmm(context.transpose(1, 2))\n",
    "\n",
    "    attention_self_mask = Variable(1 - torch.eye(words.size(1), words.size(1))).cuda()\n",
    "    attn_out = attn_out * attention_self_mask.unsqueeze(0)\n",
    "\n",
    "    out = self.softmax(attn_out)\n",
    "    out = out * att_padding_mask.float().unsqueeze(2)\n",
    "    out = out * att_padding_mask.float().unsqueeze(1)\n",
    "    out = self.att_sm_dropout(out)\n",
    "    context_v = out.bmm(word_output)\n",
    "    #context_v = self.att_dropout(context_v)\n",
    "    word_output = torch.cat([word_output, context_v], 2)\n",
    "\n",
    "    return word_output\n",
    "\n",
    "\n",
    "  def reset_parameters(self):\n",
    "\n",
    "    for name, param in self.word_lstm.named_parameters():\n",
    "        if 'bias' in name:\n",
    "            nn.init.constant(param, 0.)\n",
    "        elif 'weight' in name:\n",
    "            nn.init.normal(param, mean=0, std=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30gGnBfu_LCp"
   },
   "source": [
    "## CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNJ_5VGC_GhU"
   },
   "outputs": [],
   "source": [
    "def logsumexp(x, dim=None): #AS IS\n",
    "    if dim is None:\n",
    "        xmax = x.max()\n",
    "        xmax_ = x.max()\n",
    "        return xmax_ + torch.log(torch.exp(x - xmax).sum())\n",
    "    else:\n",
    "        xmax, _ = x.max(dim, keepdim=True)\n",
    "        xmax_, _ = x.max(dim)\n",
    "        return xmax_ + torch.log(torch.exp(x - xmax).sum(dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EFCLBuPS_Goq"
   },
   "outputs": [],
   "source": [
    "class CRF_Module(nn.Module):\n",
    "    def __init__(self, input_size, num_labels, bigram=True):\n",
    "\n",
    "\n",
    "        super(CRF_Module, self).__init__()\n",
    "        self.pad_label_id = num_labels\n",
    "        self.bigram = bigram\n",
    "        self.input_size = input_size\n",
    "        self.num_labels = num_labels + 1\n",
    "        self.state_layer = nn.Linear(input_size, self.num_labels)\n",
    "\n",
    "        if bigram: # \n",
    "            self.transition_layer = nn.Linear(input_size, self.num_labels * self.num_labels) # transition weights are learned (costs of moving from one tag to next)\n",
    "            self.register_parameter('transition_matrix', None)\n",
    "        else:\n",
    "            self.transition_layer = None\n",
    "            self.transition_matrix = Parameter(torch.Tensor(self.num_labels, self.num_labels)) # initialize a transition matrix instead \n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, input, mask=None):\n",
    "      batch, length, _ = input.size()\n",
    "      out_state = self.state_layer(input).unsqueeze(2)\n",
    "\n",
    "      if self.bigram:\n",
    "          out_transition = self.transition_layer(input).view(batch, length, self.num_labels, self.num_labels)\n",
    "          net_output = out_transition + out_state\n",
    "      else:\n",
    "          net_output = self.transition_matrix + out_state\n",
    "\n",
    "      if mask is not None:\n",
    "          net_output = net_output * mask.unsqueeze(2).unsqueeze(3)\n",
    "      return net_output\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "      nn.init.constant(self.state_layer.bias, 0.)\n",
    "      if self.bigram:\n",
    "          nn.init.xavier_uniform(self.transition_layer.weight)\n",
    "          nn.init.constant(self.transition_layer.bias, 0.)\n",
    "      else:\n",
    "          nn.init.normal(self.transition_matrix)\n",
    "\n",
    "\n",
    "    def _viterbi_decode(self, input, mask, leading_symbolic=0):\n",
    "      energy = self.forward(input, mask=mask).data\n",
    "      energyTrans = energy.transpose(0, 1) #energy_transpose\n",
    "      energyTrans = energyTrans[:, :, leading_symbolic:-1, leading_symbolic:-1]\n",
    "\n",
    "      w_len, batch_size, num_label, _ = energyTrans.size()\n",
    "      batch_index = torch.arange(0, batch_size).long().cuda()\n",
    "      \n",
    "      curr_mat = torch.zeros([w_len, batch_size, num_label, 1]).cuda()\n",
    "      pointer = torch.cuda.LongTensor(w_len, batch_size, num_label).zero_()\n",
    "      back_pointer = torch.cuda.LongTensor(w_len, batch_size).zero_()\n",
    "\n",
    "      curr_mat[0] = energy[:, 0, -1, leading_symbolic:-1].unsqueeze(2)\n",
    "      pointer[0] = -1\n",
    "      for t in range(1, w_len):\n",
    "          prev_mat = curr_mat[t - 1]\n",
    "          temp_mat, pointer[t] = torch.max(energyTrans[t] + prev_mat, dim=1)\n",
    "          curr_mat[t] = temp_mat.unsqueeze(2)\n",
    "\n",
    "      _, back_pointer[-1] = torch.max(curr_mat[-1].squeeze(2), dim=1)\n",
    "      for t in reversed(range(w_len - 1)):\n",
    "          pointer_last = pointer[t + 1]\n",
    "          back_pointer[t] = pointer_last[batch_index, back_pointer[t + 1]]\n",
    "\n",
    "      return back_pointer.transpose(0, 1) + leading_symbolic\n",
    "\n",
    "    \n",
    "    def loss(self, input, target, mask=None):\n",
    "      #debug()\n",
    "      batch, length, _ = input.size()\n",
    "      energy = self.forward(input, mask=mask)\n",
    "      energy_transpose = energy.transpose(0, 1)\n",
    "      target_transpose = target.transpose(0, 1)\n",
    "      mask_transpose = None\n",
    "      if mask is not None:\n",
    "          mask_transpose = mask.unsqueeze(2).transpose(0, 1)\n",
    "\n",
    "      partition = None\n",
    "\n",
    "      batch_index = torch.arange(0, batch).long().cuda()\n",
    "      prev_label = torch.cuda.LongTensor(batch).fill_(self.num_labels - 1)\n",
    "      tgt_energy = Variable(torch.zeros(batch)).cuda()\n",
    "\n",
    "      for t in range(length):\n",
    "          curr_energy = energy_transpose[t]\n",
    "          if t == 0:\n",
    "              partition = curr_energy[:, -1, :]\n",
    "          else:\n",
    "              partition_new = logsumexp(curr_energy + partition.unsqueeze(2), dim=1)\n",
    "              if mask_transpose is None:\n",
    "                  partition = partition_new\n",
    "              else:\n",
    "                  mask_t = mask_transpose[t]\n",
    "                  partition = partition + (partition_new - partition) * mask_t\n",
    "          tgt_energy += curr_energy[batch_index, prev_label, target_transpose[t].data]\n",
    "          prev_label = target_transpose[t].data\n",
    "\n",
    "      return logsumexp(partition, dim=1) - tgt_energy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dV2PGV_qee7p"
   },
   "source": [
    "## Wrapper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bhRc7_sveiB5"
   },
   "outputs": [],
   "source": [
    "class Attention_LSTM_CRF(nn.Module):\n",
    "    def __init__(self, char_vocab_size, char_embed_size, word_vocab_size, word_embedding_size, word_lstm_hiddden_size, word_vector, num_labels, bigram=True, char_lstm_hidden_size=50):\n",
    "        super(Attention_LSTM_CRF, self).__init__()\n",
    "        self.char_vocab_size = char_vocab_size\n",
    "        self.char_embed_size = char_embed_size\n",
    "        self.word_vocab_size = word_vocab_size\n",
    "        self.word_embedding_size = word_embedding_size\n",
    "        self.word_lstm_hiddden_size = word_lstm_hiddden_size\n",
    "        self.char_lstm_hidden_size = char_lstm_hidden_size\n",
    "        self.word_vector = word_vector\n",
    "\n",
    "        # crf vars\n",
    "        self.num_labels = num_labels\n",
    "        self.bigram = bigram\n",
    "\n",
    "        self.charModel = char_model(self.char_vocab_size, self.char_embed_size, self.char_lstm_hidden_size)\n",
    "        self.Attention = selfAttention(self.char_lstm_hidden_size, self.word_vocab_size, self.word_embedding_size, self.word_lstm_hiddden_size, self.word_vector)\n",
    "        self.CRF = CRF_Module(self.word_lstm_hiddden_size*4, num_labels)\n",
    "    \n",
    "    def forward(self, words, input, word_length, char_input_length, target, hidden=None):\n",
    "        charOut = self.charModel(input, char_input_length) #basically char_hidden_state\n",
    "        AttentionOut = self.Attention(words, charOut, word_length)\n",
    "        #debug()\n",
    "        # CRFout = self.CRF(AttentionOut, words.ne(PAD).float()).data # energy\n",
    "\n",
    "        CRFLossOut = self.CRF.loss(AttentionOut, target, words.ne(PAD).float()).mean()\n",
    "        CRFPredict = self.CRF._viterbi_decode(AttentionOut, words.ne(PAD).float(), 1)\n",
    "\n",
    "        return CRFLossOut, CRFPredict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4_Cf3Dq9QyRZ"
   },
   "source": [
    "##Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vgUnBcRlkOp"
   },
   "outputs": [],
   "source": [
    "def training(es_translation_dict , char_vocab_dict, model, optimizer, lr, epochs):\n",
    "  best_f1_score = 0.0\n",
    "  \n",
    "  for epoch in range(epochs):\n",
    "      labels_global = []\n",
    "      pred_global = []\n",
    "\n",
    "      model.train()\n",
    "\n",
    "      epoch_loss = 0\n",
    "      total = 0\n",
    "      correct = 0\n",
    "      batch =0\n",
    "      \n",
    "      \n",
    "      #es_translation_dict = english_to_spanish_dict(\"translation_bi.txt\")      \n",
    "      train_paths = ['Data/eng.train', 'Data/eng.testa', 'Data/eng.testb']\n",
    "      #char_vocab_dict = create_char_index(train_paths, es_translation_dict, pad=False)\n",
    "      sentences, OOV, labels = data_to_words_sentences(train_paths, es_translation_dict, test=False) #whole data\n",
    "      \n",
    "\n",
    "      #train_X = [words, OOV]\n",
    "      \n",
    "      for data in batch_from_data(sentences, OOV, labels, 16):\n",
    "\n",
    "          bsentences, bOOV, y = data\n",
    "          #label_input\n",
    "          \n",
    "          #[bsentences, bOOV] = X\n",
    "          word_input, word_length_input, char_input, char_length_input, label_input = get_input(word_to_index, bsentences, bOOV, char_vocab_dict, y, LABEL_INDEX)\n",
    "          optimizer.zero_grad()\n",
    "          true_labels = label_input.contiguous().view(-1)\n",
    "\n",
    "          loss, predict = model(word_input, char_input, word_length_input, char_length_input, label_input)\n",
    "          predict = predict.contiguous().view(-1)\n",
    "\n",
    "          total +=  true_labels.data.ne(PAD).float().sum()\n",
    "          pred_correct = predict.eq(true_labels.data).masked_select(true_labels.ne(PAD).data).float().sum()\n",
    "\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
    "          optimizer.step()\n",
    "\n",
    "          epoch_loss += loss.item()\n",
    "          correct += pred_correct\n",
    "          batch+=1\n",
    "\n",
    "          labels_global.extend(true_labels.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n",
    "          pred_global.extend(predict.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n",
    "          \n",
    "          # true_labels_list = list(true_labels.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n",
    "          # predict = list(predict.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n",
    "          # bsentences = [j for i in bsentences for j in i]\n",
    "          # for i in range(len(bsentences)):\n",
    "          #   print(bsentences[i] + \"\\t\" + LABEL_INDEX[true_labels_list[i]] + \"\\t\" + LABEL_INDEX[predict[i]])\n",
    "\n",
    "          if batch%200==0:\n",
    "            print(\"Batch {} loss: {:.2f}\".format(batch, epoch_loss/batch))\n",
    "    \n",
    "      f1 = f1_score(labels_global, pred_global, average='macro')\n",
    "      \n",
    "      print(\"Epoch {} training loss: {:.4f}, training accuracy: {:.4f}, f1 score {:.2f}\".format(epoch, epoch_loss/batch, correct * 100.0/total, f1))\n",
    "      lr = lr / (1.0 + epoch * 0.05) #decay=0.05\n",
    "      for param_group in optimizer.param_groups:\n",
    "          param_group['lr'] = lr\n",
    "      val_f1 = evaluate(es_translation_dict , char_vocab_dict, model)\n",
    "\n",
    "      # store at best f1\n",
    "      if val_f1 > best_f1_score :\n",
    "        best_f1_score = val_f1\n",
    "        print(\"Saving model on best val F1 so far \" + str(val_f1))\n",
    "        torch.save(model.state_dict(), \"reimplemented_baseline_bestf1.pt\")\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(es_translation_dict , char_vocab_dict, model):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    batch = 0\n",
    "\n",
    "    labels_global = []\n",
    "    pred_global = []\n",
    "\n",
    "    #es_translation_dict = english_to_spanish_dict(\"translation_bi.txt\")      \n",
    "    val_paths = ['Data/esp.testa']\n",
    "    #char_vocab_dict = create_char_index(val_paths, es_translation_dict, pad=False)\n",
    "    sentences, OOV, labels = data_to_words_sentences(val_paths, es_translation_dict, test=True) #whole data\n",
    "\n",
    "    #val_X = [words, OOV]\n",
    "    for data in batch_from_data(sentences, OOV, labels, 16):\n",
    "        \n",
    "        bsentences, bOOV, y = data\n",
    "        #label_input\n",
    "        \n",
    "        #[bsentences, bOOV] = X\n",
    "        \n",
    "        word_input, word_length_input, char_input, char_length_input, label_input = get_input(word_to_index, bsentences, bOOV, char_vocab_dict, y, LABEL_INDEX)\n",
    "        loss, predict = model(word_input, char_input, word_length_input, char_length_input, label_input)\n",
    "        predict = predict.contiguous().view(-1)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        true_labels = label_input.contiguous().view(-1)\n",
    "        total += true_labels.data.ne(PAD).float().sum()\n",
    "        pred_correct = predict.eq(true_labels.data).masked_select(true_labels.ne(PAD).data).float().sum()\n",
    "        correct += pred_correct\n",
    "        batch+=1\n",
    "\n",
    "        labels_global.extend(true_labels.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n",
    "        pred_global.extend(predict.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n",
    "\n",
    "        # true_labels_list = list(true_labels.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n",
    "        # predict = list(predict.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n",
    "        # bsentences = [j for i in bsentences for j in i]\n",
    "        # for i in range(len(bsentences)):\n",
    "        #   print(bsentences[i] + \"\\t\" + LABEL_INDEX[true_labels_list[i]] + \"\\t\" + LABEL_INDEX[predict[i]])\n",
    "\n",
    "    test_acc = correct * 100.0 / total\n",
    "\n",
    "    f1 = f1_score(labels_global, pred_global, average='macro')\n",
    "\n",
    "    print(\"loss: {:.4f} eval acc: {:.4f} | f1 {:.4f}\".format(test_loss/batch, test_acc, f1))\n",
    "    return f1\n",
    "\n",
    "def write_to_results(es_translation_dict , char_vocab_dict, model):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    labels_global = []\n",
    "    pred_global = []\n",
    "   \n",
    "    test_paths = ['Data/esp.train', 'Data/esp.testb']\n",
    "    sentences, OOV, labels = data_to_words_sentences(test_paths, es_translation_dict, test=True) #whole data\n",
    "\n",
    "    for data in batch_from_data(sentences, OOV, labels, 16, random=False):\n",
    "        # debug()\n",
    "        bsentences, bOOV, y = data\n",
    "        \n",
    "        word_input, word_length_input, char_input, char_length_input, label_input = get_input(word_to_index, bsentences, bOOV, char_vocab_dict, y, LABEL_INDEX)\n",
    "        loss, predict = model(word_input, char_input, word_length_input, char_length_input, label_input)\n",
    "        predict = predict.contiguous().view(-1)\n",
    "\n",
    "        true_labels = label_input.contiguous().view(-1)\n",
    "\n",
    "        labels_global.extend(list(true_labels.masked_select(true_labels.ne(PAD).data).cpu().data.numpy()))\n",
    "        pred_global.extend(list(predict.masked_select(true_labels.ne(PAD).data).cpu().data.numpy()))\n",
    "        # true_labels_list = list(true_labels.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n",
    "        # predict = list(predict.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n",
    "        # bsentences = [j for i in bsentences for j in i]\n",
    "        # for i in range(len(bsentences)):\n",
    "        #   print(bsentences[i] + \"\\t\" + LABEL_INDEX[true_labels_list[i]] + \"\\t\" + LABEL_INDEX[predict[i]])\n",
    "\n",
    "    sentence_tokens = [j for i in sentences for j in i]\n",
    "\n",
    "    with open(\"results.txt\", \"w\") as f:\n",
    "        for i in range(len(sentence_tokens)):\n",
    "            f.write(sentence_tokens[i] + \"\\t\" + LABEL_INDEX[labels_global[i]] + \"\\t\" + LABEL_INDEX[pred_global[i]] + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IoY8DwYOb3Cf"
   },
   "source": [
    "#Training and Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gRB0YA1ub-pa"
   },
   "source": [
    "##Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8Oc394u5D6n"
   },
   "outputs": [],
   "source": [
    "train_paths = ['Data/eng.train', 'Data/eng.testa', 'Data/eng.testb']\n",
    "val_paths = ['Data/esp.testa']\n",
    "test_paths = ['Data/esp.train', 'Data/esp.testb']\n",
    "\n",
    "\n",
    "es_translation_dict = english_to_spanish_dict(\"translations_bi.txt\") \n",
    "all_paths=[]\n",
    "all_paths.extend(train_paths)\n",
    "all_paths.extend(val_paths)\n",
    "all_paths.extend(test_paths)  \n",
    "char_vocab_dict = create_char_index(all_paths, es_translation_dict, pad=False)\n",
    "matrix, word_to_index = get_indexed_word_embeddings(\"translations_bi.txt\", \"spanish.glove.gigaword_wiki.100d.magnitude\") #word_vector, word_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLPDlaYkb4g2"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "ZqsWcyRUFT6U",
    "outputId": "40c6bf96-1544-4928-a0a3-17edd0433eba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200 loss: 6.41\n",
      "Batch 400 loss: 4.77\n",
      "Batch 600 loss: 4.07\n",
      "Batch 800 loss: 3.63\n",
      "Batch 1000 loss: 3.32\n",
      "Batch 1200 loss: 3.09\n",
      "Epoch 0 training loss: 3.0017, training accuracy: 91.4249, f1 score 0.57\n",
      "loss: 5.0168 eval acc: 92.0734 | f1 0.4887\n",
      "Saving model on best val F1 so far 0.4886933952154568\n",
      "Batch 200 loss: 1.61\n",
      "Batch 400 loss: 1.58\n",
      "Batch 600 loss: 1.54\n",
      "Batch 800 loss: 1.50\n",
      "Batch 1000 loss: 1.47\n",
      "Batch 1200 loss: 1.44\n",
      "Epoch 1 training loss: 1.4191, training accuracy: 95.6980, f1 score 0.77\n",
      "loss: 5.1074 eval acc: 92.5401 | f1 0.5383\n",
      "Saving model on best val F1 so far 0.538305830505521\n",
      "Batch 200 loss: 1.02\n",
      "Batch 400 loss: 1.01\n",
      "Batch 600 loss: 1.00\n",
      "Batch 800 loss: 0.98\n",
      "Batch 1000 loss: 0.98\n",
      "Batch 1200 loss: 0.97\n",
      "Epoch 2 training loss: 0.9697, training accuracy: 97.1389, f1 score 0.85\n",
      "loss: 4.9409 eval acc: 92.4815 | f1 0.5514\n",
      "Saving model on best val F1 so far 0.5513649003628972\n"
     ]
    }
   ],
   "source": [
    "model = Attention_LSTM_CRF(len(char_vocab_dict), 25, len(matrix), 100, 200, matrix, len(LABEL_INDEX)).cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.015, momentum=0.9)\n",
    "\n",
    "training(es_translation_dict, char_vocab_dict, model, optimizer, 0.015, 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iA9mq4rcFi3"
   },
   "source": [
    "##saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ubq9FvBkDEJq"
   },
   "outputs": [],
   "source": [
    "## saving code integrated above in training loop\n",
    "# torch.save(model.state_dict(), \"reimplemented_baseline_batch16.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kaLzuUxG3h4d"
   },
   "source": [
    "## Open model and write to results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "JcB4w59H3deH",
    "outputId": "8fe5246a-83be-4778-df69-03f682ede8a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    }
   ],
   "source": [
    "model = Attention_LSTM_CRF(len(char_vocab_dict), 25, len(matrix), 100, 200, matrix, len(LABEL_INDEX)).cuda()\n",
    "model.load_state_dict(torch.load(\"reimplemented_baseline_bestf1.pt\"))\n",
    "\n",
    "write_to_results(es_translation_dict, char_vocab_dict, model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Published Baseline Reimplementation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
