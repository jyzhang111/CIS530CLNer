{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Extension_Categories_NEWNEW.ipynb","provenance":[{"file_id":"1mLk-8hRUyRfh5TtFrY7W9W6Ei-KdSKhU","timestamp":1588275145619},{"file_id":"128ZLOE4-Qg6kFQ-3WhDPpVaInkh8mSe8","timestamp":1587912228858}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bJWDq-8og-mX","colab_type":"text"},"source":["# pytorch setup, imports and constants initialization"]},{"cell_type":"code","metadata":{"id":"fYAFESqjg8fM","colab_type":"code","outputId":"3ae9db86-b624-4893-e922-378ce2e64bec","executionInfo":{"status":"ok","timestamp":1588275304542,"user_tz":240,"elapsed":122446,"user":{"displayName":"John Zhang","photoUrl":"","userId":"01738065264018885336"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["from os.path import exists\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","!pip3 install https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl\n","!pip3 install torch torchvision\n","  \n","import torch\n","device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting torch==1.0.1\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl (614.8MB)\n","\u001b[K     |████████████████████████████████| 614.8MB 28kB/s \n","\u001b[31mERROR: torchvision 0.6.0+cu101 has requirement torch==1.5.0, but you'll have torch 1.0.1 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: torch\n","  Found existing installation: torch 1.5.0+cu101\n","    Uninstalling torch-1.5.0+cu101:\n","      Successfully uninstalled torch-1.5.0+cu101\n","Successfully installed torch-1.0.1\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.3)\n","cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NK4-jWrThJd5","colab_type":"code","outputId":"91593adc-5628-4bcd-92ce-a3370deb25fd","executionInfo":{"status":"ok","timestamp":1588275356801,"user_tz":240,"elapsed":172249,"user":{"displayName":"John Zhang","photoUrl":"","userId":"01738065264018885336"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["!pip3 install pymagnitude\n","from pymagnitude import *\n","\n","import time\n","import argparse\n","import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torch import optim\n","from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n","from sklearn.metrics import f1_score\n","from pdb import set_trace as debug\n","\n","# Flag, set to True if you want to use DE data as well for pretraining\n","use_de = True\n","\n","PAD = 0\n","UNK = 1\n","BOS = 2\n","EOS = 3\n","\n","PAD_WORD = '<blank>'\n","UNK_WORD = '<unk>'\n","BOS_WORD = '<s>'\n","EOS_WORD = '</s>'\n","\n","CAT = ['PER', 'ORG', 'LOC', 'MISC']\n","POSITION = ['I', 'B']\n","LABEL_INDEX = [PAD_WORD] + ['O'] + [\"{}-{}\".format(position, cat) for cat in CAT for position in POSITION]\n","\n","train_paths = ['Data/eng.train', 'Data/eng.testa', 'Data/eng.testb']\n","if use_de: train_paths.extend(['Data/ned.train', 'Data/ned.testa', 'Data/ned.testb'])\n","val_paths = ['Data/esp.testa']\n","test_paths = ['Data/esp.train', 'Data/esp.testb']"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting pymagnitude\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/a3/b9a34d22ed8c0ed59b00ff55092129641cdfa09d82f9abdc5088051a5b0c/pymagnitude-0.1.120.tar.gz (5.4MB)\n","\u001b[K     |████████████████████████████████| 5.4MB 2.8MB/s \n","\u001b[?25hBuilding wheels for collected packages: pymagnitude\n","  Building wheel for pymagnitude (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pymagnitude: filename=pymagnitude-0.1.120-cp36-cp36m-linux_x86_64.whl size=135918206 sha256=5a475fc9e35999bd120b478d83e1d194cb129f8327181fccdd4d32f9e9fa3b43\n","  Stored in directory: /root/.cache/pip/wheels/a2/c7/98/cb48b9db35f8d1a7827b764dc36c5515179dc116448a47c8a1\n","Successfully built pymagnitude\n","Installing collected packages: pymagnitude\n","Successfully installed pymagnitude-0.1.120\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vvgAcjm6hyiH","colab_type":"text"},"source":["# Connect to Google Drive"]},{"cell_type":"code","metadata":{"id":"0YfhNBjSynDh","colab_type":"code","outputId":"83609234-87d8-4987-b8c4-e0dfc8ada2a9","executionInfo":{"status":"ok","timestamp":1588275184045,"user_tz":240,"elapsed":16033,"user":{"displayName":"John Zhang","photoUrl":"","userId":"01738065264018885336"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","%cd \"/content/gdrive/Shared drives/CIS 530 Project/\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","/content/gdrive/Shared drives/CIS 530 Project\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vZxser1ahU72","colab_type":"text"},"source":["# Utility Functions"]},{"cell_type":"code","metadata":{"id":"hhafrdBfhXZo","colab_type":"code","colab":{}},"source":["# Returns the dictionary from the file with translations\n","def language_to_spanish_dict(path):\n","  l_to_spanish = dict()\n","\n","  with open(path, 'r') as f:\n","    for line in f:\n","        line = line.rstrip('\\n').split('\\t')\n","        en_word = line[0]\n","        es_word = line[1]\n","        if en_word.isupper(): l_to_spanish[en_word] = es_word.upper()\n","        elif len(en_word) > 0 and en_word[0].isupper(): l_to_spanish[en_word] = es_word.capitalize()\n","        else: l_to_spanish[en_word] = es_word\n","\n","  return l_to_spanish\n","\n","# Returns a matrix of category distributions of the translation word, seen in\n","# the training set, or for all test words the closes translation word\n","def previous_ner_distributions(embedding_matrix, word_to_index, en_to_spanish, de_to_spanish=None):\n","  embedding_size = embedding_matrix.shape[1]\n","  translation_words = []\n","  translation_words_matrix = []\n","  category_distribution_matrix = np.zeros((len(word_to_index), 5))\n","\n","  # Columns = O, PER, ORG, LOC, MISC\n","  en_files = [f for f in train_paths if 'eng' in f]\n","  for file in en_files:\n","    with open(file, 'r') as f:\n","      for line in f:\n","        line = line.rstrip('\\n')\n","        if line == \"\": continue\n","\n","        word = line.split()[0]\n","        trans = en_to_spanish[word]\n","        tag = line.split()[3]\n","        if trans not in translation_words:\n","          translation_words.append(trans)\n","          translation_words_matrix.append(embedding_matrix[word_to_index[trans], :])\n","        col = 0\n","        if 'PER' in tag: col = 1\n","        elif 'ORG' in tag: col = 2\n","        elif 'LOC' in tag: col = 3\n","        elif 'MISC' in tag: col = 4\n","        category_distribution_matrix[word_to_index[trans], col] += 1\n","\n","  if de_to_spanish:\n","    de_files = [f for f in train_paths if 'ned' in f]\n","    for file in de_files:\n","      with open(file, 'r') as f:\n","        for line in f:\n","          line = line.rstrip('\\n')\n","          if line == \"\": continue\n","\n","          word = line.split()[0]\n","          trans = de_to_spanish[word]\n","          tag = line.split()[2]\n","          if trans not in translation_words:\n","            translation_words.append(trans)\n","            translation_words_matrix.append(embedding_matrix[word_to_index[trans], :])\n","          col = 0\n","          if 'PER' in tag: col = 1\n","          elif 'ORG' in tag: col = 2\n","          elif 'LOC' in tag: col = 3\n","          elif 'MISC' in tag: col = 4\n","          category_distribution_matrix[word_to_index[trans], col] += 1\n","\n","  translation_words_matrix = np.array(translation_words_matrix)\n","  norms = np.linalg.norm(translation_words_matrix, axis=1)\n","  # ratios on each row\n","  sums = np.maximum(category_distribution_matrix.sum(axis=1), 1)\n","  category_distribution_matrix = category_distribution_matrix / sums[:, None]\n","\n","  es_files = val_paths + test_paths\n","\n","  for file in es_files:\n","    with open(file, 'r') as f:\n","      for line in f:\n","        line = line.rstrip('\\n')\n","        if line == \"\": continue\n","\n","        word = line.split()[0]\n","        if category_distribution_matrix[word_to_index[word], :].sum() == 0:\n","          embedding = embedding_matrix[word_to_index[word], :]\n","          embedding = np.reshape(embedding, (embedding_size, 1))\n","          closeness = np.matmul(translation_words_matrix, embedding)/norms[:, None]\n","          closest_distribution = category_distribution_matrix[word_to_index[translation_words[np.argmax(closeness)]], :]\n","          category_distribution_matrix[word_to_index[word], :] = closest_distribution\n","\n","  return category_distribution_matrix\n","\n","def get_category_average_word_embeddings(matrix, en_to_spanish, word_to_index, de_to_spanish=None):\n","  CAT_TO_IDX = {'PER':0, 'ORG':1, 'LOC':2, 'MISC':3, 'O':4}\n","  CAT = ['PER', 'ORG', 'LOC', 'MISC', 'O']\n","  dim = matrix.shape[1]\n","\n","  CAT_EMB = np.array([[0 for i in range(dim)] for j in range(len(CAT))], dtype=np.float64)\n","  \n","  cnt = [0]*5\n","  en_files = [f for f in train_paths if 'eng' in f]\n","  for file in en_files:\n","    with open(file, 'r') as f:\n","      for line in f:\n","        line = line.rstrip('\\n')\n","        if line == \"\": continue\n","\n","        word = line.split()[0]\n","        cat = line.split()[3].replace('I-','').replace('B-','')\n","        trans = en_to_spanish[word]\n","\n","        emb = np.squeeze(matrix[word_to_index[trans], :])\n","\n","        CAT_EMB[CAT_TO_IDX[cat]] += emb\n","        cnt[CAT_TO_IDX[cat]]+=1\n","\n","  if de_to_spanish:\n","    de_files = [f for f in train_paths if 'ned' in f]\n","    for file in de_files:\n","      with open(file, 'r') as f:\n","        for line in f:\n","          line = line.rstrip('\\n')\n","          if line == \"\": continue\n","\n","          word = line.split()[0]\n","          cat = line.split()[2].replace('I-','').replace('B-','')\n","          trans = de_to_spanish[word]\n","\n","          emb = np.squeeze(matrix[word_to_index[trans], :])\n","\n","          CAT_EMB[CAT_TO_IDX[cat]] += emb\n","          cnt[CAT_TO_IDX[cat]]+=1\n","\n","  cnt = np.array(cnt)\n","  cnt = cnt[:,np.newaxis]\n","  CAT_EMB = np.divide(CAT_EMB,cnt)\n","  return CAT_EMB\n","\n","def find_distance(dim, cat_emb, emb):\n","  dis = []\n","  for i in range(len(cat_emb)):\n","    dis.append(np.inner(cat_emb[i], emb) / (np.linalg.norm(cat_emb[i]) * np.linalg.norm(emb)))\n","\n","  return dis\n","\n","def all_distances(matrix, word_to_index, en_to_spanish, de_to_spanish=None):\n","  dim = matrix.shape[1]\n","  all_category_distances = []\n","\n","  cat_emb = get_category_average_word_embeddings(matrix, en_to_spanish, word_to_index, de_to_spanish)\n","  for i in range(matrix.shape[0]):\n","    all_category_distances.append(find_distance(dim, cat_emb, matrix[i, :]))\n","  return np.array(all_category_distances)\n","\n","def get_distributional_info(embedding_matrix, word_to_index, en_to_spanish, distance=True, de_to_spanish=None):\n","  if distance: return all_distances(embedding_matrix, word_to_index, en_to_spanish, de_to_spanish)\n","  else: return previous_ner_distributions(embedding_matrix, word_to_index, en_to_spanish, de_to_spanish)\n","\n","# Returns a tuple (matrix, word_to_index) where the matrix contains all the \n","# X-dimensional word embeddings, and word_to_index is a dictionary from the\n","# word into the index in the matrix. For Out-of-vocabulary words it creates\n","# a random embedding\n","def get_indexed_word_embeddings(en_to_spanish, embedding_path, de_to_spanish=None):\n","  vectors = Magnitude(embedding_path)\n","\n","  dim = vectors.dim\n","  \n","  matrix = [] # words by embedding-dimension\n","  word_to_index = dict()\n","\n","  index = 0\n","\n","  en_files = [f for f in train_paths if 'eng' in f]\n","  \n","  for file in en_files:\n","    with open(file, 'r') as f:\n","      for line in f:\n","        line = line.rstrip('\\n')\n","        if line == \"\": continue\n","\n","        word = line.split()[0]\n","        trans = en_to_spanish[word]\n","        if trans not in word_to_index:\n","          word_to_index[trans] = index\n","          if trans in vectors: matrix.append(vectors.query(trans))\n","          else: matrix.append(np.random.uniform(-(3/dim)**0.5, (3/dim)**0.5, dim))\n","\n","          index += 1\n","\n","  if de_to_spanish:\n","    de_files = [f for f in train_paths if 'ned' in f]\n","    for file in de_files:\n","      with open(file, 'r') as f:\n","        for line in f:\n","          line = line.rstrip('\\n')\n","          if line == \"\": continue\n","\n","          word = line.split()[0]\n","          trans = de_to_spanish[word]\n","          if trans not in word_to_index:\n","            word_to_index[trans] = index\n","            if trans in vectors: matrix.append(vectors.query(trans))\n","            else: matrix.append(np.random.uniform(-(3/dim)**0.5, (3/dim)**0.5, dim))\n","\n","            index += 1\n","\n","  es_files = val_paths + test_paths\n","  \n","  for file in es_files:\n","    with open(file, 'r') as f:\n","      for line in f:\n","        line = line.rstrip('\\n')\n","        if line == \"\": continue\n","\n","        word = line.split()[0]\n","        if word not in word_to_index:\n","          word_to_index[word] = index\n","          if word in vectors: matrix.append(vectors.query(word))\n","          else: matrix.append(np.random.uniform(-(3/dim)**0.5, (3/dim)**0.5, dim))\n","            \n","          index += 1\n","\n","  return np.array(matrix), word_to_index\n","\n","#returns char dictionary created from all path in paths\n","def create_char_index(paths, en_to_spanish, pad=False, de_to_spanish=None):\n","    char_dict = {}\n","    if pad:\n","        char_dict[PAD_WORD] = PAD\n","        char_dict[UNK_WORD] = UNK\n","    else:\n","        char_dict[UNK_WORD] = 0\n","\n","    for path in paths:\n","        for line in open(path):\n","            l = line.strip().split()\n","            if len(l) > 0:# and l[0] != '':\n","              #l[0] is word l[1] is POS, l[2] is gold standard NER label\n","              word = l[0]\n","              es_word = word\n","              if 'eng' in path and word in en_to_spanish:\n","                es_word = en_to_spanish.get(word)\n","              if de_to_spanish:\n","                if 'ned' in path and word in de_to_spanish:\n","                  es_word = de_to_spanish.get(word)\n","              for i in range(len(es_word)):\n","                  if es_word[i] not in char_dict:\n","                      char_dict[es_word[i]] = len(char_dict)\n","\n","    return char_dict\n","\n","#   Returns \n","#1. all the spanish 'sentences' in 2D array\n","#2. 2D array indicating if word in given sentence is OOV word (True if the word is used as-is, translation not found) or not\n","#3. 2D array returning labels.\n","#   in the file(s) at paths in an array.\n","def data_to_words_sentences(paths, en_to_spanish, test=False, de_to_spanish=None):\n","  sentences=[]\n","  curr_sentence=[]\n","  OOV = []\n","  labels=[]\n","  curr_OOV_sentence = []\n","  curr_label_sentence = []\n","  word_idx = 0\n","  for path in paths:\n","      for line in open(path):\n","        line = line.strip().split()\n","        \n","        end_of_line=False\n","        if len(line) == 0:\n","          end_of_line=True\n","        \n","        if not end_of_line:\n","          word = line[0]\n","          es_word = word\n","\n","          if not test:\n","            if 'eng' in path: curr_label_sentence.append(line[3]) #english has 4th column label\n","            else: curr_label_sentence.append(line[2]) #dutch has 3rd column label\n","          else: #spanish has 3rd column label\n","            curr_label_sentence.append(line[2])\n","\n","          curr_OOV_sentence.append(True)\n","          if test:\n","            curr_OOV_sentence[word_idx] = False\n","          else: #not test\n","            if 'eng' in path and word in en_to_spanish:\n","              es_word = en_to_spanish.get(word)\n","              curr_OOV_sentence[word_idx] = False\n","            elif de_to_spanish:\n","              if word in de_to_spanish:\n","                es_word = de_to_spanish.get(word)\n","                curr_OOV_sentence[word_idx] = False\n","          \n","          word_idx = word_idx+1\n","          curr_sentence.append(es_word)\n","        if end_of_line:\n","          sentences.append(curr_sentence)\n","          OOV.append(curr_OOV_sentence)\n","          labels.append(curr_label_sentence)\n","          curr_sentence=[]\n","          curr_OOV_sentence=[]\n","          curr_label_sentence=[]\n","          word_idx=0\n","  return sentences, OOV, labels\n","\n","#ADD all methods to get initial input to neural network here. Returns char, labels and word input\n","#OOV[i] indicates if word[i] is OOV (translation not found)\n","def get_input(word_vocab_dict, sentences, OOV, char_vocab_dict, token_labels, label_to_index):\n","  max_word_len = max(len(word) for sentence in sentences for word in sentence)\n","  #max_word_len = max(len(word) for word in words)\n","  max_sentence_length = max(len(sentence) for sentence in sentences)\n","\n","  word_input = np.zeros((len(sentences), max_sentence_length), dtype='int64')\n","  word_input_length = [len(sentence) for sentence in sentences]\n","\n","  char_input = np.zeros((len(sentences), max_sentence_length,max_word_len), dtype='int64') \n","  char_input_length = np.zeros((len(sentences), max_sentence_length), dtype='int64') #2D array of length of word in each sentence in sentences\n","  \n","  label_input = np.zeros((len(sentences), max_sentence_length), dtype='int64') #2D array of label of word in each sentence in sentences\n","  for i in range(len(sentences)):\n","    for j in range(len(sentences[i])):\n","      word_input[i][j] = word_vocab_dict[sentences[i][j]]\n","      char_input_length[i][j] = len(sentences[i][j]) \n","      label_input[i][j] = label_to_index.index(token_labels[i][j])\n","      for k in range(len(sentences[i][j])):\n","        c = sentences[i][j][k]\n","        if c in char_vocab_dict:\n","          input_zero = c.isdigit() or OOV[i][j]\n","          char_input[i][j][k] = char_vocab_dict['0' if input_zero else c]\n","        else:\n","          char_input[i][j][k] = UNK\n","\n","  word_input_var = Variable(torch.from_numpy(word_input), requires_grad=False)\n","  word_input_length_var = Variable(torch.LongTensor(word_input_length), requires_grad=False)\n","  label_var = Variable(torch.from_numpy(label_input), requires_grad=False)\n","  char_input_var = Variable(torch.from_numpy(char_input), requires_grad=False)\n","  char_input_length_var = Variable(torch.from_numpy(char_input_length), requires_grad=False)\n","  return word_input_var.cuda(), word_input_length_var.cuda(), char_input_var.cuda(), char_input_length_var.cuda(), label_var.cuda()\n","\n","def batch_from_data(X, X1, y, batch_size, random=True):\n","    batch_num = int(np.ceil(len(y) / float(batch_size)))\n","    rand_indices = np.arange(len(y))\n","    if random: rand_indices = np.random.permutation(len(y))\n","\n","    for batch in range(0, batch_num):\n","        bs = batch_size if batch < batch_num - 1 else len(y) - batch_size * batch\n","        #from pdb import set_trace as debug\n","        #debug()\n","        yield [X[i] for i in rand_indices[batch * batch_size : batch * batch_size + bs]], [X1[i] for i in rand_indices[batch * batch_size : batch * batch_size + bs]], [y[i] for i in rand_indices[batch * batch_size : batch * batch_size + bs]]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wpY4fK3phkHJ","colab_type":"text"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"gzIbKaR0bph_","colab_type":"text"},"source":["##char embedding"]},{"cell_type":"code","metadata":{"id":"FPyhlkQQhmfg","colab_type":"code","colab":{}},"source":["class char_model(nn.Module):\n","    def __init__(self, char_vocab_size, char_embed_size, char_lstm_hidden_size=50):\n","\n","        #START: char embedding section\n","        super(char_model, self).__init__()\n","        self.char_embed = nn.Embedding(char_vocab_size, char_embed_size, padding_idx=PAD)\n","        self.char_lstm = nn.LSTM(char_embed_size, char_lstm_hidden_size, bidirectional=True, batch_first=True)\n","        self.char_lstm_hidden_size = char_lstm_hidden_size\n","        #END: char embedding section \n","\n","    def forward(self, char_inp, char_input_length):\n","\n","        #START: char embedding section\n","        #from pdb import set_trace as debug\n","        #debug()\n","        char_input = char_inp.view(-1, char_inp.size(2))\n","        char_input_length_sorted, char_original_idx = char_input_length.view(-1).sort(0, descending=True)\n","        char_embedded = self.char_embed(char_input)\n","        char_embedded_sorted = char_embedded[char_original_idx] #get embeddings in descending order of length of word in words\n","\n","        char_input_length_sorted_size = char_input_length_sorted.size(0)\n","        last_index = char_input_length_sorted_size\n","        if char_input_length_sorted.data.eq(0).sum() != 0: #atleast 1 element of char_input_length_sorted is/are zero\n","          last_index = char_input_length_sorted.data.eq(0).nonzero()[0][0]\n","        char_embedded_sorted = char_embedded_sorted[:last_index]\n","        char_input_length_sorted = char_input_length_sorted[:last_index]\n","\n","        char_input_packed_padded = pack_padded_sequence(char_embedded_sorted, char_input_length_sorted.cpu().data.numpy(), batch_first=True)\n","        char_output_packed_padded, (h_n, c_n) = self.char_lstm(char_input_packed_padded)\n","        char_hidden_state = torch.cat([h_n[0], h_n[1]], 1)\n","\n","        if last_index != char_input_length_sorted_size:\n","          zero_padding_diff = char_input_length_sorted_size - last_index\n","          zero_padding = Variable(torch.zeros((zero_padding_diff, 2*self.char_lstm_hidden_size)), requires_grad=False).cuda()\n","          char_hidden_state = torch.cat([char_hidden_state, zero_padding], 0)\n","\n","        char_hidden_state = char_hidden_state[torch.argsort(char_original_idx)] #char_hidden_state[torch.from_numpy(np.argsort(char_original_idx.cpu().data.numpy())).cuda()]\n","        char_hidden_state = char_hidden_state.view(char_inp.size(0), -1, char_hidden_state.size(1))\n","        return char_hidden_state\n","        #END: char embedding section \n","\n","    def reset_parameters(self):\n","\n","      #START: char embedding section\n","      for param in self.char_embed.parameters():\n","          nn.init.normal(param, mean=0, std=0.01)\n","\n","      for name, param in self.char_lstm.named_parameters():\n","          if 'bias' in name:\n","              nn.init.constant_(param, 0.)\n","          elif 'weight' in name:\n","              nn.init.normal(param, mean=0, std=0.1)\n","      #END: char embedding section "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DY26JE0ebtmV","colab_type":"text"},"source":["##Self Attention"]},{"cell_type":"code","metadata":{"id":"evmwaqT6-fal","colab_type":"code","colab":{}},"source":["class selfAttention(nn.Module):\n","\n","  def __init__(self, char_lstm_hidden_size, word_vocab_size, word_embedding_size, word_lstm_hiddden_size, word_vector):\n","    super(selfAttention, self).__init__()\n","    self.word_embed = nn.Embedding(word_vocab_size, word_embedding_size, padding_idx=PAD)\n","    self.word_lstm = nn.LSTM(2*char_lstm_hidden_size + word_embedding_size, word_lstm_hiddden_size, batch_first=True, bidirectional=True)\n","\n","    self.word_linear = nn.Linear(word_lstm_hiddden_size * 2, word_lstm_hiddden_size * 2)\n","\n","    self.tanh = nn.Tanh()\n","    self.softmax = nn.Softmax(dim=2)\n","    self.embedding_dropout = nn.Dropout(0.5)\n","    self.word_dropout = nn.Dropout(0.5)\n","    self.att_sm_dropout = nn.Dropout(0.5)\n","    #self.att_dropout= nn.Dropout(0.2)\n","    self.word_embed.weight.data.copy_(torch.from_numpy(np.asarray(word_vector)))\n","\n","\n","\n","\n","  def forward(self, words, char_hidden_state, word_length):\n","    word_embedding = self.word_embed(words)\n","    word_lstm_input = torch.cat([word_embedding, char_hidden_state], 2)\n","\n","    word_lstm_input = self.embedding_dropout(word_lstm_input)\n","    \n","    word_length, word_idx = word_length.sort(0, descending=True)\n","    word_lstm_input = word_lstm_input[word_idx]\n","\n","    word_packed_input = pack_padded_sequence(word_lstm_input, word_length.cpu().data.numpy(), batch_first=True)\n","    word_packed_output, _ = self.word_lstm(word_packed_input)\n","    word_output, _ = pad_packed_sequence(word_packed_output, batch_first=True)\n","    word_output = word_output[torch.from_numpy(np.argsort(word_idx.cpu().data.numpy())).cuda()]\n","    \n","    word_output = self.word_dropout(word_output)\n","    attn_input = self.tanh(self.word_linear(word_output))\n","\n","    att_padding_mask = Variable(words.data.ne(PAD)).cuda()\n","    context = attn_input * att_padding_mask.float().unsqueeze(2)\n","    attn_out = context.bmm(context.transpose(1, 2))\n","\n","    attention_self_mask = Variable(1 - torch.eye(words.size(1), words.size(1))).cuda()\n","    attn_out = attn_out * attention_self_mask.unsqueeze(0)\n","\n","    out = self.softmax(attn_out)\n","    out = out * att_padding_mask.float().unsqueeze(2)\n","    out = out * att_padding_mask.float().unsqueeze(1)\n","    out = self.att_sm_dropout(out)\n","    context_v = out.bmm(word_output)\n","    #context_v = self.att_dropout(context_v)\n","    word_output = torch.cat([word_output, context_v], 2)\n","\n","    return word_output\n","\n","\n","  def reset_parameters(self):\n","\n","    for name, param in self.word_lstm.named_parameters():\n","        if 'bias' in name:\n","            nn.init.constant(param, 0.)\n","        elif 'weight' in name:\n","            nn.init.normal(param, mean=0, std=0.1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"30gGnBfu_LCp","colab_type":"text"},"source":["## CRF"]},{"cell_type":"code","metadata":{"id":"fNJ_5VGC_GhU","colab_type":"code","colab":{}},"source":["def logsumexp(x, dim=None): #AS IS\n","    if dim is None:\n","        xmax = x.max()\n","        xmax_ = x.max()\n","        return xmax_ + torch.log(torch.exp(x - xmax).sum())\n","    else:\n","        xmax, _ = x.max(dim, keepdim=True)\n","        xmax_, _ = x.max(dim)\n","        return xmax_ + torch.log(torch.exp(x - xmax).sum(dim))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFCLBuPS_Goq","colab_type":"code","colab":{}},"source":["class CRF_Module(nn.Module):\n","    def __init__(self, input_size, num_labels, bigram=True):\n","\n","\n","        super(CRF_Module, self).__init__()\n","        self.pad_label_id = num_labels\n","        self.bigram = bigram\n","        self.input_size = input_size\n","        self.num_labels = num_labels + 1\n","        self.state_layer = nn.Linear(input_size, self.num_labels)\n","\n","        if bigram: # \n","            self.transition_layer = nn.Linear(input_size, self.num_labels * self.num_labels) # transition weights are learned (costs of moving from one tag to next)\n","            self.register_parameter('transition_matrix', None)\n","        else:\n","            self.transition_layer = None\n","            self.transition_matrix = Parameter(torch.Tensor(self.num_labels, self.num_labels)) # initialize a transition matrix instead \n","\n","        self.reset_parameters()\n","\n","    def forward(self, input, mask=None):\n","      batch, length, _ = input.size()\n","      out_state = self.state_layer(input).unsqueeze(2)\n","\n","      if self.bigram:\n","          out_transition = self.transition_layer(input).view(batch, length, self.num_labels, self.num_labels)\n","          net_output = out_transition + out_state\n","      else:\n","          net_output = self.transition_matrix + out_state\n","\n","      if mask is not None:\n","          net_output = net_output * mask.unsqueeze(2).unsqueeze(3)\n","      return net_output\n","\n","\n","    def reset_parameters(self):\n","      nn.init.constant(self.state_layer.bias, 0.)\n","      if self.bigram:\n","          nn.init.xavier_uniform(self.transition_layer.weight)\n","          nn.init.constant(self.transition_layer.bias, 0.)\n","      else:\n","          nn.init.normal(self.transition_matrix)\n","\n","\n","    def _viterbi_decode(self, input, mask, leading_symbolic=0):\n","      energy = self.forward(input, mask=mask).data\n","      energyTrans = energy.transpose(0, 1) #energy_transpose\n","      energyTrans = energyTrans[:, :, leading_symbolic:-1, leading_symbolic:-1]\n","\n","      w_len, batch_size, num_label, _ = energyTrans.size()\n","      batch_index = torch.arange(0, batch_size).long().cuda()\n","      \n","      curr_mat = torch.zeros([w_len, batch_size, num_label, 1]).cuda()\n","      pointer = torch.cuda.LongTensor(w_len, batch_size, num_label).zero_()\n","      back_pointer = torch.cuda.LongTensor(w_len, batch_size).zero_()\n","\n","      curr_mat[0] = energy[:, 0, -1, leading_symbolic:-1].unsqueeze(2)\n","      pointer[0] = -1\n","      for t in range(1, w_len):\n","          prev_mat = curr_mat[t - 1]\n","          temp_mat, pointer[t] = torch.max(energyTrans[t] + prev_mat, dim=1)\n","          curr_mat[t] = temp_mat.unsqueeze(2)\n","\n","      _, back_pointer[-1] = torch.max(curr_mat[-1].squeeze(2), dim=1)\n","      for t in reversed(range(w_len - 1)):\n","          pointer_last = pointer[t + 1]\n","          back_pointer[t] = pointer_last[batch_index, back_pointer[t + 1]]\n","\n","      return back_pointer.transpose(0, 1) + leading_symbolic\n","\n","    \n","    def loss(self, input, target, mask=None):\n","      #debug()\n","      batch, length, _ = input.size()\n","      energy = self.forward(input, mask=mask)\n","      energy_transpose = energy.transpose(0, 1)\n","      target_transpose = target.transpose(0, 1)\n","      mask_transpose = None\n","      if mask is not None:\n","          mask_transpose = mask.unsqueeze(2).transpose(0, 1)\n","\n","      partition = None\n","\n","      batch_index = torch.arange(0, batch).long().cuda()\n","      prev_label = torch.cuda.LongTensor(batch).fill_(self.num_labels - 1)\n","      tgt_energy = Variable(torch.zeros(batch)).cuda()\n","\n","      for t in range(length):\n","          curr_energy = energy_transpose[t]\n","          if t == 0:\n","              partition = curr_energy[:, -1, :]\n","          else:\n","              partition_new = logsumexp(curr_energy + partition.unsqueeze(2), dim=1)\n","              if mask_transpose is None:\n","                  partition = partition_new\n","              else:\n","                  mask_t = mask_transpose[t]\n","                  partition = partition + (partition_new - partition) * mask_t\n","          tgt_energy += curr_energy[batch_index, prev_label, target_transpose[t].data]\n","          prev_label = target_transpose[t].data\n","\n","      return logsumexp(partition, dim=1) - tgt_energy\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dV2PGV_qee7p","colab_type":"text"},"source":["## wrapper code"]},{"cell_type":"code","metadata":{"id":"bhRc7_sveiB5","colab_type":"code","colab":{}},"source":["class Attention_LSTM_CRF(nn.Module):\n","    def __init__(self, char_vocab_size, char_embed_size, word_vocab_size, word_embedding_size, word_lstm_hiddden_size, word_vector, num_labels, cat_vector=None, cat=0, bigram=True, char_lstm_hidden_size=50):\n","        super(Attention_LSTM_CRF, self).__init__()\n","        self.char_vocab_size = char_vocab_size\n","        self.char_embed_size = char_embed_size\n","        self.word_vocab_size = word_vocab_size\n","        self.word_embedding_size = word_embedding_size\n","        self.word_lstm_hiddden_size = word_lstm_hiddden_size\n","        self.char_lstm_hidden_size = char_lstm_hidden_size\n","        self.word_vector = word_vector\n","\n","\n","        self.cat = 0\n","        if cat_vector is not None:\n","          self.cat_vector = cat_vector\n","          self.cat_embed = nn.Embedding(word_vocab_size, cat, padding_idx=PAD)\n","          self.cat_embed.weight.data.copy_(torch.from_numpy(np.asarray(cat_vector)))\n","          self.cat = cat\n","\n","\n","        # crf vars\n","        self.num_labels = num_labels\n","        self.bigram = bigram\n","\n","        self.charModel = char_model(self.char_vocab_size, self.char_embed_size, self.char_lstm_hidden_size)\n","        self.Attention = selfAttention(self.char_lstm_hidden_size, self.word_vocab_size, self.word_embedding_size, self.word_lstm_hiddden_size, self.word_vector)\n","        self.CRF = CRF_Module(self.word_lstm_hiddden_size*4+cat, num_labels)\n","    \n","    def forward(self, words, input, word_length, char_input_length, target, hidden=None):\n","        charOut = self.charModel(input, char_input_length) #basically char_hidden_state\n","        AttentionOut = self.Attention(words, charOut, word_length)\n","        #Concat with category embedding\n","        if self.cat!=0:\n","          cat_embedding = self.cat_embed(words)\n","          AttentionOut = torch.cat([AttentionOut, cat_embedding], 2)\n","        #debug()\n","        # CRFout = self.CRF(AttentionOut, words.ne(PAD).float()).data # energy\n","\n","        CRFLossOut = self.CRF.loss(AttentionOut, target, words.ne(PAD).float()).mean()\n","        CRFPredict = self.CRF._viterbi_decode(AttentionOut, words.ne(PAD).float(), 1)\n","\n","        return CRFLossOut, CRFPredict"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4_Cf3Dq9QyRZ","colab_type":"text"},"source":["##Training and Validation"]},{"cell_type":"code","metadata":{"id":"8vgUnBcRlkOp","colab_type":"code","colab":{}},"source":["def training(en_to_spanish , char_vocab_dict, model, optimizer, lr, epochs, de_to_spanish=None):\n","  best_f1_score = 0.0\n","  \n","  for epoch in range(epochs):\n","      labels_global = []\n","      pred_global = []\n","\n","      model.train()\n","\n","      epoch_loss = 0\n","      total = 0\n","      correct = 0\n","      batch =0\n","      \n","      sentences, OOV, labels = data_to_words_sentences(train_paths, en_to_spanish, False, de_to_spanish) #whole data\n","      \n","      for data in batch_from_data(sentences, OOV, labels, 16):\n","\n","          bsentences, bOOV, y = data\n","          #label_input\n","          \n","          #[bsentences, bOOV] = X\n","          word_input, word_length_input, char_input, char_length_input, label_input = get_input(word_to_index, bsentences, bOOV, char_vocab_dict, y, LABEL_INDEX)\n","          optimizer.zero_grad()\n","          true_labels = label_input.contiguous().view(-1)\n","\n","          loss, predict = model(word_input, char_input, word_length_input, char_length_input, label_input)\n","          predict = predict.contiguous().view(-1)\n","\n","          total +=  true_labels.data.ne(PAD).float().sum()\n","          pred_correct = predict.eq(true_labels.data).masked_select(true_labels.ne(PAD).data).float().sum()\n","\n","          loss.backward()\n","          torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n","          optimizer.step()\n","\n","          epoch_loss += loss.item()\n","          correct += pred_correct\n","          batch+=1\n","\n","          labels_global.extend(true_labels.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n","          pred_global.extend(predict.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n","          \n","          # true_labels_list = list(true_labels.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n","          # predict = list(predict.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n","          # bsentences = [j for i in bsentences for j in i]\n","          # for i in range(len(bsentences)):\n","          #   print(bsentences[i] + \"\\t\" + LABEL_INDEX[true_labels_list[i]] + \"\\t\" + LABEL_INDEX[predict[i]])\n","\n","          if batch%200==0:\n","            print(\"Batch {} loss: {:.2f}\".format(batch, epoch_loss/batch))\n","    \n","      f1 = f1_score(labels_global, pred_global, average='macro')\n","      \n","      print(\"Epoch {} training loss: {:.4f}, training accuracy: {:.4f}, f1 score {:.2f}\".format(epoch, epoch_loss/batch, correct * 100.0/total, f1))\n","      lr = lr / (1.0 + epoch * 0.05) #decay=0.05\n","      for param_group in optimizer.param_groups:\n","          param_group['lr'] = lr\n","      val_f1 = evaluate(en_to_spanish, char_vocab_dict, model, de_to_spanish)\n","\n","      # store at best f1\n","      if val_f1 > best_f1_score :\n","        best_f1_score = val_f1\n","        print(\"Saving model on best val F1 so far \" + str(val_f1))\n","        torch.save(model.state_dict(), \"reimplemented_baseline_bestf1.pt\")\n","\n","def evaluate(en_to_spanish, char_vocab_dict, model, de_to_spanish = None):\n","\n","    model.eval()\n","\n","    correct = 0\n","    total = 0\n","    test_loss = 0\n","    batch = 0\n","\n","    labels_global = []\n","    pred_global = []\n","    \n","    sentences, OOV, labels = data_to_words_sentences(val_paths, en_to_spanish, True, de_to_spanish) #whole data\n","\n","    #val_X = [words, OOV]\n","    for data in batch_from_data(sentences, OOV, labels, 16):\n","        \n","        bsentences, bOOV, y = data\n","        #label_input\n","        \n","        #[bsentences, bOOV] = X\n","        \n","        word_input, word_length_input, char_input, char_length_input, label_input = get_input(word_to_index, bsentences, bOOV, char_vocab_dict, y, LABEL_INDEX)\n","        loss, predict = model(word_input, char_input, word_length_input, char_length_input, label_input)\n","        predict = predict.contiguous().view(-1)\n","        test_loss += loss.item()\n","\n","        true_labels = label_input.contiguous().view(-1)\n","        total += true_labels.data.ne(PAD).float().sum()\n","        pred_correct = predict.eq(true_labels.data).masked_select(true_labels.ne(PAD).data).float().sum()\n","        correct += pred_correct\n","        batch+=1\n","\n","        labels_global.extend(true_labels.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n","        pred_global.extend(predict.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n","\n","        # true_labels_list = list(true_labels.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n","        # predict = list(predict.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n","        # bsentences = [j for i in bsentences for j in i]\n","        # for i in range(len(bsentences)):\n","        #   print(bsentences[i] + \"\\t\" + LABEL_INDEX[true_labels_list[i]] + \"\\t\" + LABEL_INDEX[predict[i]])\n","\n","    test_acc = correct * 100.0 / total\n","\n","    f1 = f1_score(labels_global, pred_global, average='macro')\n","\n","    print(\"loss: {:.4f} eval acc: {:.4f} | f1 {:.4f}\".format(test_loss/batch, test_acc, f1))\n","    return f1\n","\n","def write_to_results(en_to_spanish, char_vocab_dict, model, de_to_spanish=None):\n","\n","    model.eval()\n","\n","    labels_global = []\n","    pred_global = []\n","   \n","    sentences, OOV, labels = data_to_words_sentences(test_paths, en_to_spanish, True, de_to_spanish) #whole data\n","\n","    for data in batch_from_data(sentences, OOV, labels, 16, random=False):\n","        # debug()\n","        bsentences, bOOV, y = data\n","        \n","        word_input, word_length_input, char_input, char_length_input, label_input = get_input(word_to_index, bsentences, bOOV, char_vocab_dict, y, LABEL_INDEX)\n","        loss, predict = model(word_input, char_input, word_length_input, char_length_input, label_input)\n","        predict = predict.contiguous().view(-1)\n","\n","        true_labels = label_input.contiguous().view(-1)\n","\n","        labels_global.extend(list(true_labels.masked_select(true_labels.ne(PAD).data).cpu().data.numpy()))\n","        pred_global.extend(list(predict.masked_select(true_labels.ne(PAD).data).cpu().data.numpy()))\n","        # true_labels_list = list(true_labels.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n","        # predict = list(predict.masked_select(true_labels.ne(PAD).data).cpu().data.numpy())\n","        # bsentences = [j for i in bsentences for j in i]\n","        # for i in range(len(bsentences)):\n","        #   print(bsentences[i] + \"\\t\" + LABEL_INDEX[true_labels_list[i]] + \"\\t\" + LABEL_INDEX[predict[i]])\n","\n","    sentence_tokens = [j for i in sentences for j in i]\n","\n","    with open(\"results_new_edit.txt\", \"w\") as f:\n","        for i in range(len(sentence_tokens)):\n","            f.write(sentence_tokens[i] + \"\\t\" + LABEL_INDEX[labels_global[i]] + \"\\t\" + LABEL_INDEX[pred_global[i]] + \"\\n\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IoY8DwYOb3Cf","colab_type":"text"},"source":["#Training and Evaluating"]},{"cell_type":"markdown","metadata":{"id":"gRB0YA1ub-pa","colab_type":"text"},"source":["##Loading data"]},{"cell_type":"code","metadata":{"id":"Z8Oc394u5D6n","colab_type":"code","colab":{}},"source":["# spanish.glove.gigaword_wiki.100d.magnitude\n","embedding_path = \"spanish.glove.gigaword_wiki.100d.magnitude\"\n","\n","path = \"translations_bi.txt\"\n","de_es_translation_dict = None\n","if use_de: \n","  embedding_path = \"umwe-esp-smallish.magnitude\"\n","  de_es_translation_dict = language_to_spanish_dict(\"translations_umwe_ned.txt\")\n","  path = \"translations_umwe_eng.txt\"\n","en_es_translation_dict = language_to_spanish_dict(path)\n","all_paths=[]\n","all_paths.extend(train_paths)\n","all_paths.extend(val_paths)\n","all_paths.extend(test_paths)  \n","char_vocab_dict = create_char_index(all_paths, en_es_translation_dict, False, de_es_translation_dict)\n","matrix, word_to_index = get_indexed_word_embeddings(en_es_translation_dict, embedding_path, de_es_translation_dict) #word_vector, word_\n","\n","use_distances = True\n","\n","extraInfo = None\n","numCat = 0\n","if use_distances:\n","  # change to True to use distances to category means, False to use ratio of previous distributions\n","  extraInfo = get_distributional_info(matrix, word_to_index, en_es_translation_dict, True, de_es_translation_dict)\n","  numCat = len(CAT) + 1\n","\n","matrix = np.concatenate((matrix, extraInfo), axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qLPDlaYkb4g2","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"ZqsWcyRUFT6U","colab_type":"code","outputId":"3af4f78f-c713-43e1-9a8f-468a9ba2399f","executionInfo":{"status":"ok","timestamp":1588280745346,"user_tz":240,"elapsed":5522575,"user":{"displayName":"John Zhang","photoUrl":"","userId":"01738065264018885336"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = Attention_LSTM_CRF(len(char_vocab_dict), 25, len(matrix), 305, 200, matrix, len(LABEL_INDEX), None, 0).cuda()\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.015, momentum=0.9)\n","\n","training(en_es_translation_dict, char_vocab_dict, model, optimizer, 0.015, 30, de_es_translation_dict)\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"],"name":"stderr"},{"output_type":"stream","text":["Batch 200 loss: 5.64\n","Batch 400 loss: 4.07\n","Batch 600 loss: 3.45\n","Batch 800 loss: 3.07\n","Batch 1000 loss: 2.81\n","Batch 1200 loss: 2.64\n","Batch 1400 loss: 2.50\n","Batch 1600 loss: 2.37\n","Batch 1800 loss: 2.28\n","Batch 2000 loss: 2.19\n","Batch 2200 loss: 2.12\n","Batch 2400 loss: 2.05\n","Batch 2600 loss: 1.99\n","Epoch 0 training loss: 1.9394, training accuracy: 94.0562, f1 score 0.60\n","loss: 5.5841 eval acc: 89.7814 | f1 0.4742\n","Saving model on best val F1 so far 0.4741690841688502\n","Batch 200 loss: 0.96\n","Batch 400 loss: 0.98\n","Batch 600 loss: 0.96\n","Batch 800 loss: 0.95\n","Batch 1000 loss: 0.94\n","Batch 1200 loss: 0.94\n","Batch 1400 loss: 0.93\n","Batch 1600 loss: 0.91\n","Batch 1800 loss: 0.89\n","Batch 2000 loss: 0.89\n","Batch 2200 loss: 0.88\n","Batch 2400 loss: 0.87\n","Batch 2600 loss: 0.86\n","Epoch 1 training loss: 0.8606, training accuracy: 97.2047, f1 score 0.80\n","loss: 4.7005 eval acc: 92.6062 | f1 0.5618\n","Saving model on best val F1 so far 0.5618189065907966\n","Batch 200 loss: 0.55\n","Batch 400 loss: 0.58\n","Batch 600 loss: 0.57\n","Batch 800 loss: 0.57\n","Batch 1000 loss: 0.57\n","Batch 1200 loss: 0.57\n","Batch 1400 loss: 0.56\n","Batch 1600 loss: 0.57\n","Batch 1800 loss: 0.56\n","Batch 2000 loss: 0.56\n","Batch 2200 loss: 0.56\n","Batch 2400 loss: 0.56\n","Batch 2600 loss: 0.56\n","Epoch 2 training loss: 0.5564, training accuracy: 98.1908, f1 score 0.87\n","loss: 4.9625 eval acc: 91.9714 | f1 0.5611\n","Batch 200 loss: 0.42\n","Batch 400 loss: 0.41\n","Batch 600 loss: 0.39\n","Batch 800 loss: 0.39\n","Batch 1000 loss: 0.40\n","Batch 1200 loss: 0.40\n","Batch 1400 loss: 0.40\n","Batch 1600 loss: 0.39\n","Batch 1800 loss: 0.39\n","Batch 2000 loss: 0.39\n","Batch 2200 loss: 0.40\n","Batch 2400 loss: 0.40\n","Batch 2600 loss: 0.40\n","Epoch 3 training loss: 0.3979, training accuracy: 98.7182, f1 score 0.90\n","loss: 4.9580 eval acc: 92.9275 | f1 0.5823\n","Saving model on best val F1 so far 0.5823153094539415\n","Batch 200 loss: 0.32\n","Batch 400 loss: 0.32\n","Batch 600 loss: 0.32\n","Batch 800 loss: 0.33\n","Batch 1000 loss: 0.32\n","Batch 1200 loss: 0.32\n","Batch 1400 loss: 0.32\n","Batch 1600 loss: 0.31\n","Batch 1800 loss: 0.31\n","Batch 2000 loss: 0.31\n","Batch 2200 loss: 0.32\n","Batch 2400 loss: 0.32\n","Batch 2600 loss: 0.32\n","Epoch 4 training loss: 0.3157, training accuracy: 98.9858, f1 score 0.93\n","loss: 5.1462 eval acc: 92.6969 | f1 0.5826\n","Saving model on best val F1 so far 0.5825915475201164\n","Batch 200 loss: 0.24\n","Batch 400 loss: 0.25\n","Batch 600 loss: 0.24\n","Batch 800 loss: 0.24\n","Batch 1000 loss: 0.24\n","Batch 1200 loss: 0.25\n","Batch 1400 loss: 0.25\n","Batch 1600 loss: 0.25\n","Batch 1800 loss: 0.25\n","Batch 2000 loss: 0.25\n","Batch 2200 loss: 0.25\n","Batch 2400 loss: 0.26\n","Batch 2600 loss: 0.25\n","Epoch 5 training loss: 0.2540, training accuracy: 99.1823, f1 score 0.94\n","loss: 5.5775 eval acc: 91.8901 | f1 0.5508\n","Batch 200 loss: 0.20\n","Batch 400 loss: 0.21\n","Batch 600 loss: 0.21\n","Batch 800 loss: 0.21\n","Batch 1000 loss: 0.21\n","Batch 1200 loss: 0.21\n","Batch 1400 loss: 0.21\n","Batch 1600 loss: 0.21\n","Batch 1800 loss: 0.21\n","Batch 2000 loss: 0.21\n","Batch 2200 loss: 0.21\n","Batch 2400 loss: 0.21\n","Batch 2600 loss: 0.22\n","Epoch 6 training loss: 0.2162, training accuracy: 99.2883, f1 score 0.95\n","loss: 5.3747 eval acc: 92.5401 | f1 0.5759\n","Batch 200 loss: 0.16\n","Batch 400 loss: 0.16\n","Batch 600 loss: 0.18\n","Batch 800 loss: 0.18\n","Batch 1000 loss: 0.18\n","Batch 1200 loss: 0.19\n","Batch 1400 loss: 0.19\n","Batch 1600 loss: 0.19\n","Batch 1800 loss: 0.19\n","Batch 2000 loss: 0.19\n","Batch 2200 loss: 0.18\n","Batch 2400 loss: 0.18\n","Batch 2600 loss: 0.18\n","Epoch 7 training loss: 0.1814, training accuracy: 99.4053, f1 score 0.96\n","loss: 5.7093 eval acc: 92.3625 | f1 0.5947\n","Saving model on best val F1 so far 0.5947464886457017\n","Batch 200 loss: 0.16\n","Batch 400 loss: 0.17\n","Batch 600 loss: 0.17\n","Batch 800 loss: 0.16\n","Batch 1000 loss: 0.16\n","Batch 1200 loss: 0.16\n","Batch 1400 loss: 0.16\n","Batch 1600 loss: 0.16\n","Batch 1800 loss: 0.16\n","Batch 2000 loss: 0.16\n","Batch 2200 loss: 0.16\n","Batch 2400 loss: 0.16\n","Batch 2600 loss: 0.16\n","Epoch 8 training loss: 0.1604, training accuracy: 99.4770, f1 score 0.96\n","loss: 5.5867 eval acc: 92.7555 | f1 0.5831\n","Batch 200 loss: 0.13\n","Batch 400 loss: 0.14\n","Batch 600 loss: 0.13\n","Batch 800 loss: 0.14\n","Batch 1000 loss: 0.14\n","Batch 1200 loss: 0.14\n","Batch 1400 loss: 0.14\n","Batch 1600 loss: 0.14\n","Batch 1800 loss: 0.14\n","Batch 2000 loss: 0.14\n","Batch 2200 loss: 0.14\n","Batch 2400 loss: 0.14\n","Batch 2600 loss: 0.14\n","Epoch 9 training loss: 0.1423, training accuracy: 99.5292, f1 score 0.97\n","loss: 5.6740 eval acc: 92.9860 | f1 0.5954\n","Saving model on best val F1 so far 0.5954497237873626\n","Batch 200 loss: 0.16\n","Batch 400 loss: 0.14\n","Batch 600 loss: 0.14\n","Batch 800 loss: 0.14\n","Batch 1000 loss: 0.13\n","Batch 1200 loss: 0.13\n","Batch 1400 loss: 0.13\n","Batch 1600 loss: 0.13\n","Batch 1800 loss: 0.13\n","Batch 2000 loss: 0.13\n","Batch 2200 loss: 0.13\n","Batch 2400 loss: 0.13\n","Batch 2600 loss: 0.13\n","Epoch 10 training loss: 0.1288, training accuracy: 99.5711, f1 score 0.97\n","loss: 5.6085 eval acc: 92.7763 | f1 0.6010\n","Saving model on best val F1 so far 0.6010400024966175\n","Batch 200 loss: 0.13\n","Batch 400 loss: 0.13\n","Batch 600 loss: 0.12\n","Batch 800 loss: 0.12\n","Batch 1000 loss: 0.12\n","Batch 1200 loss: 0.12\n","Batch 1400 loss: 0.12\n","Batch 1600 loss: 0.12\n","Batch 1800 loss: 0.12\n","Batch 2000 loss: 0.12\n","Batch 2200 loss: 0.12\n","Batch 2400 loss: 0.12\n","Batch 2600 loss: 0.12\n","Epoch 11 training loss: 0.1232, training accuracy: 99.5848, f1 score 0.97\n","loss: 5.7253 eval acc: 93.1278 | f1 0.6029\n","Saving model on best val F1 so far 0.6028782848230878\n","Batch 200 loss: 0.11\n","Batch 400 loss: 0.11\n","Batch 600 loss: 0.11\n","Batch 800 loss: 0.11\n","Batch 1000 loss: 0.12\n","Batch 1200 loss: 0.12\n","Batch 1400 loss: 0.11\n","Batch 1600 loss: 0.11\n","Batch 1800 loss: 0.12\n","Batch 2000 loss: 0.12\n","Batch 2200 loss: 0.12\n","Batch 2400 loss: 0.12\n","Batch 2600 loss: 0.12\n","Epoch 12 training loss: 0.1174, training accuracy: 99.6076, f1 score 0.97\n","loss: 5.7938 eval acc: 92.8575 | f1 0.5969\n","Batch 200 loss: 0.12\n","Batch 400 loss: 0.11\n","Batch 600 loss: 0.11\n","Batch 800 loss: 0.11\n","Batch 1000 loss: 0.11\n","Batch 1200 loss: 0.11\n","Batch 1400 loss: 0.11\n","Batch 1600 loss: 0.11\n","Batch 1800 loss: 0.11\n","Batch 2000 loss: 0.11\n","Batch 2200 loss: 0.11\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 13 training loss: 0.1129, training accuracy: 99.6184, f1 score 0.97\n","loss: 5.8751 eval acc: 92.7895 | f1 0.5961\n","Batch 200 loss: 0.11\n","Batch 400 loss: 0.11\n","Batch 600 loss: 0.11\n","Batch 800 loss: 0.11\n","Batch 1000 loss: 0.10\n","Batch 1200 loss: 0.10\n","Batch 1400 loss: 0.10\n","Batch 1600 loss: 0.10\n","Batch 1800 loss: 0.10\n","Batch 2000 loss: 0.10\n","Batch 2200 loss: 0.11\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 14 training loss: 0.1077, training accuracy: 99.6294, f1 score 0.97\n","loss: 5.8770 eval acc: 92.7971 | f1 0.5970\n","Batch 200 loss: 0.11\n","Batch 400 loss: 0.12\n","Batch 600 loss: 0.11\n","Batch 800 loss: 0.11\n","Batch 1000 loss: 0.11\n","Batch 1200 loss: 0.11\n","Batch 1400 loss: 0.11\n","Batch 1600 loss: 0.11\n","Batch 1800 loss: 0.11\n","Batch 2000 loss: 0.11\n","Batch 2200 loss: 0.11\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 15 training loss: 0.1096, training accuracy: 99.6392, f1 score 0.97\n","loss: 5.8744 eval acc: 92.8500 | f1 0.5964\n","Batch 200 loss: 0.10\n","Batch 400 loss: 0.10\n","Batch 600 loss: 0.10\n","Batch 800 loss: 0.10\n","Batch 1000 loss: 0.10\n","Batch 1200 loss: 0.10\n","Batch 1400 loss: 0.10\n","Batch 1600 loss: 0.11\n","Batch 1800 loss: 0.11\n","Batch 2000 loss: 0.11\n","Batch 2200 loss: 0.10\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 16 training loss: 0.1071, training accuracy: 99.6603, f1 score 0.98\n","loss: 5.8663 eval acc: 92.8632 | f1 0.5966\n","Batch 200 loss: 0.11\n","Batch 400 loss: 0.11\n","Batch 600 loss: 0.11\n","Batch 800 loss: 0.11\n","Batch 1000 loss: 0.11\n","Batch 1200 loss: 0.11\n","Batch 1400 loss: 0.11\n","Batch 1600 loss: 0.11\n","Batch 1800 loss: 0.11\n","Batch 2000 loss: 0.11\n","Batch 2200 loss: 0.11\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 17 training loss: 0.1085, training accuracy: 99.6245, f1 score 0.97\n","loss: 5.8805 eval acc: 92.8368 | f1 0.5977\n","Batch 200 loss: 0.11\n","Batch 400 loss: 0.10\n","Batch 600 loss: 0.10\n","Batch 800 loss: 0.10\n","Batch 1000 loss: 0.10\n","Batch 1200 loss: 0.10\n","Batch 1400 loss: 0.11\n","Batch 1600 loss: 0.11\n","Batch 1800 loss: 0.11\n","Batch 2000 loss: 0.11\n","Batch 2200 loss: 0.11\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 18 training loss: 0.1091, training accuracy: 99.6290, f1 score 0.97\n","loss: 5.9002 eval acc: 92.7971 | f1 0.5962\n","Batch 200 loss: 0.10\n","Batch 400 loss: 0.11\n","Batch 600 loss: 0.11\n","Batch 800 loss: 0.11\n","Batch 1000 loss: 0.11\n","Batch 1200 loss: 0.11\n","Batch 1400 loss: 0.11\n","Batch 1600 loss: 0.11\n","Batch 1800 loss: 0.11\n","Batch 2000 loss: 0.11\n","Batch 2200 loss: 0.11\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 19 training loss: 0.1089, training accuracy: 99.6344, f1 score 0.98\n","loss: 5.8849 eval acc: 92.8198 | f1 0.5965\n","Batch 200 loss: 0.10\n","Batch 400 loss: 0.09\n","Batch 600 loss: 0.10\n","Batch 800 loss: 0.10\n","Batch 1000 loss: 0.10\n","Batch 1200 loss: 0.10\n","Batch 1400 loss: 0.10\n","Batch 1600 loss: 0.10\n","Batch 1800 loss: 0.11\n","Batch 2000 loss: 0.11\n","Batch 2200 loss: 0.11\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 20 training loss: 0.1069, training accuracy: 99.6439, f1 score 0.98\n","loss: 5.8834 eval acc: 92.8216 | f1 0.5966\n","Batch 200 loss: 0.11\n","Batch 400 loss: 0.11\n","Batch 600 loss: 0.10\n","Batch 800 loss: 0.10\n","Batch 1000 loss: 0.11\n","Batch 1200 loss: 0.11\n","Batch 1400 loss: 0.11\n","Batch 1600 loss: 0.11\n","Batch 1800 loss: 0.11\n","Batch 2000 loss: 0.11\n","Batch 2200 loss: 0.11\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 21 training loss: 0.1084, training accuracy: 99.6303, f1 score 0.97\n","loss: 5.9008 eval acc: 92.8349 | f1 0.5970\n","Batch 200 loss: 0.11\n","Batch 400 loss: 0.11\n","Batch 600 loss: 0.11\n","Batch 800 loss: 0.11\n","Batch 1000 loss: 0.11\n","Batch 1200 loss: 0.11\n","Batch 1400 loss: 0.10\n","Batch 1600 loss: 0.10\n","Batch 1800 loss: 0.10\n","Batch 2000 loss: 0.10\n","Batch 2200 loss: 0.10\n","Batch 2400 loss: 0.10\n","Batch 2600 loss: 0.11\n","Epoch 22 training loss: 0.1058, training accuracy: 99.6359, f1 score 0.98\n","loss: 5.8830 eval acc: 92.8273 | f1 0.5968\n","Batch 200 loss: 0.10\n","Batch 400 loss: 0.11\n","Batch 600 loss: 0.11\n","Batch 800 loss: 0.11\n","Batch 1000 loss: 0.12\n","Batch 1200 loss: 0.11\n","Batch 1400 loss: 0.12\n","Batch 1600 loss: 0.11\n","Batch 1800 loss: 0.11\n","Batch 2000 loss: 0.11\n","Batch 2200 loss: 0.11\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 23 training loss: 0.1092, training accuracy: 99.6277, f1 score 0.97\n","loss: 5.8953 eval acc: 92.8254 | f1 0.5967\n","Batch 200 loss: 0.11\n","Batch 400 loss: 0.11\n","Batch 600 loss: 0.12\n","Batch 800 loss: 0.11\n","Batch 1000 loss: 0.11\n","Batch 1200 loss: 0.11\n","Batch 1400 loss: 0.11\n","Batch 1600 loss: 0.11\n","Batch 1800 loss: 0.11\n","Batch 2000 loss: 0.11\n","Batch 2200 loss: 0.11\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 24 training loss: 0.1093, training accuracy: 99.6246, f1 score 0.97\n","loss: 5.9054 eval acc: 92.8292 | f1 0.5969\n","Batch 200 loss: 0.11\n","Batch 400 loss: 0.11\n","Batch 600 loss: 0.10\n","Batch 800 loss: 0.10\n","Batch 1000 loss: 0.10\n","Batch 1200 loss: 0.10\n","Batch 1400 loss: 0.10\n","Batch 1600 loss: 0.10\n","Batch 1800 loss: 0.10\n","Batch 2000 loss: 0.10\n","Batch 2200 loss: 0.11\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 25 training loss: 0.1062, training accuracy: 99.6462, f1 score 0.98\n","loss: 5.9016 eval acc: 92.8292 | f1 0.5965\n","Batch 200 loss: 0.11\n","Batch 400 loss: 0.11\n","Batch 600 loss: 0.11\n","Batch 800 loss: 0.10\n","Batch 1000 loss: 0.11\n","Batch 1200 loss: 0.10\n","Batch 1400 loss: 0.10\n","Batch 1600 loss: 0.10\n","Batch 1800 loss: 0.10\n","Batch 2000 loss: 0.11\n","Batch 2200 loss: 0.10\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 26 training loss: 0.1078, training accuracy: 99.6361, f1 score 0.98\n","loss: 5.8916 eval acc: 92.8235 | f1 0.5968\n","Batch 200 loss: 0.10\n","Batch 400 loss: 0.11\n","Batch 600 loss: 0.11\n","Batch 800 loss: 0.11\n","Batch 1000 loss: 0.11\n","Batch 1200 loss: 0.11\n","Batch 1400 loss: 0.11\n","Batch 1600 loss: 0.11\n","Batch 1800 loss: 0.11\n","Batch 2000 loss: 0.11\n","Batch 2200 loss: 0.11\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 27 training loss: 0.1066, training accuracy: 99.6369, f1 score 0.98\n","loss: 5.8903 eval acc: 92.8254 | f1 0.5969\n","Batch 200 loss: 0.10\n","Batch 400 loss: 0.10\n","Batch 600 loss: 0.11\n","Batch 800 loss: 0.11\n","Batch 1000 loss: 0.11\n","Batch 1200 loss: 0.11\n","Batch 1400 loss: 0.11\n","Batch 1600 loss: 0.11\n","Batch 1800 loss: 0.11\n","Batch 2000 loss: 0.11\n","Batch 2200 loss: 0.11\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 28 training loss: 0.1063, training accuracy: 99.6434, f1 score 0.98\n","loss: 5.8861 eval acc: 92.8311 | f1 0.5970\n","Batch 200 loss: 0.12\n","Batch 400 loss: 0.11\n","Batch 600 loss: 0.11\n","Batch 800 loss: 0.11\n","Batch 1000 loss: 0.11\n","Batch 1200 loss: 0.11\n","Batch 1400 loss: 0.11\n","Batch 1600 loss: 0.11\n","Batch 1800 loss: 0.11\n","Batch 2000 loss: 0.11\n","Batch 2200 loss: 0.11\n","Batch 2400 loss: 0.11\n","Batch 2600 loss: 0.11\n","Epoch 29 training loss: 0.1118, training accuracy: 99.6246, f1 score 0.97\n","loss: 5.8886 eval acc: 92.8330 | f1 0.5972\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0iA9mq4rcFi3","colab_type":"text"},"source":["##saving model"]},{"cell_type":"code","metadata":{"id":"Ubq9FvBkDEJq","colab_type":"code","colab":{}},"source":["## saving code integrated above in training loop\n","# torch.save(model.state_dict(), \"reimplemented_baseline_batch16.pt\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kaLzuUxG3h4d","colab_type":"text"},"source":["## Open model and write to results"]},{"cell_type":"code","metadata":{"id":"JcB4w59H3deH","colab_type":"code","colab":{}},"source":["# model = Attention_LSTM_CRF(len(char_vocab_dict), 25, len(matrix), 100, 200, matrix, len(LABEL_INDEX), extraInfo, numCat).cuda()\n","# model.load_state_dict(torch.load(\"reimplemented_baseline_bestf1.pt\"))\n","\n","write_to_results(en_es_translation_dict, char_vocab_dict, model, de_es_translation_dict)"],"execution_count":0,"outputs":[]}]}